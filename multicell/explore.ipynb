{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Testing multicell functionality\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with own multicell data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# path hack for relative import in jupyter notebook\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# LIBRARY GLOBAL MODS\n",
    "CELLTYPES = os.path.dirname(os.path.abspath(''))\n",
    "sys.path.append(CELLTYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import umap\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})\n",
    "\n",
    "from utils.file_io import RUNS_FOLDER\n",
    "\n",
    "# TODO 1: bokeh plot with s%d\n",
    "# TODO 2: visualize s%d, get energy of s%d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_OUTDIR = RUNS_FOLDER + os.sep + 'explore' + os.sep + 'nb_explore'\n",
    "os.makedirs(NOTEBOOK_OUTDIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_control_data(total_spins, num_runs):\n",
    "    X_01 = np.random.randint(2, size=(num_runs, total_spins))\n",
    "    X = X_01 * 2 - 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_seed = 100\n",
    "# see defaults: https://umap-learn.readthedocs.io/en/latest/api.html\n",
    "umap_kwargs = {\n",
    "    'random_state': umap_seed,\n",
    "    'n_components': 3,\n",
    "    'metric': 'euclidean',  \n",
    "    'init': 'spectral',\n",
    "    'min_dist': 0.1,\n",
    "    'spread': 1.0,\n",
    "}\n",
    "\n",
    "#gamma_vals = ['0e','0.05e', '0.1e', '0.2e', '1e', '2e','20e']\n",
    "gamma_vals = ['0e_10k','0.05e_10k', '0.1e_10k', '0.2e_10k', '1e_10k', '2e_10k', '20e_10k']\n",
    "manyruns_datadirs = [RUNS_FOLDER + os.sep + 'multicell_manyruns_gamma%s' % gamma for gamma in gamma_vals]\n",
    "datasets = {i: {'label': 'gamma_%s' % gamma_vals[i]} for i in range(len(gamma_vals))}\n",
    "\n",
    "for idx in range(len(gamma_vals)):\n",
    "    fpath_state = manyruns_datadirs[idx] + os.sep + 'aggregate' + os.sep + 'X_aggregate.npz'\n",
    "    fpath_energy = manyruns_datadirs[idx] + os.sep + 'aggregate' + os.sep + 'X_energy.npz'\n",
    "    fpath_pickle = manyruns_datadirs[idx] + os.sep + 'multicell_template.pkl'\n",
    "    print(fpath_state)\n",
    "    X = np.load(fpath_state)['arr_0'].T               # umap wants transpose\n",
    "    X_energies = np.load(fpath_energy)['arr_0'].T     # umap wants transpose (?)\n",
    "    with open(fpath_pickle, 'rb') as pickle_file:\n",
    "        multicell_template = pickle.load(pickle_file)  # unpickling multicell object\n",
    "    \n",
    "    # store data and metadata in datasets object\n",
    "    num_runs, total_spins = X.shape\n",
    "    print(X.shape)\n",
    "    datasets[idx]['data'] = X\n",
    "    datasets[idx]['index'] = list(range(num_runs))\n",
    "    datasets[idx]['energies'] = X_energies\n",
    "    datasets[idx]['num_runs'] = num_runs\n",
    "    datasets[idx]['total_spins'] = total_spins\n",
    "    datasets[idx]['multicell_template'] = multicell_template\n",
    "    \n",
    "    # perform dimension reduction\n",
    "    t1 = time.time()\n",
    "    datasets[idx]['reducer'] = umap.UMAP(**umap_kwargs)\n",
    "    datasets[idx]['reducer'].fit(X)\n",
    "    datasets[idx]['embedding'] = datasets[idx]['reducer'].transform(X)\n",
    "    print('Time to fit: %.2f sec' % (time.time() - t1))\n",
    "    \n",
    "    # Verify that the result of calling transform is\n",
    "    # idenitical to accessing the embedding_ attribute\n",
    "    assert(np.all(datasets[idx]['embedding'] == datasets[idx]['reducer'].embedding_))\n",
    "    print('embedding.shape:', datasets[idx]['embedding'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_spins_0 = datasets[0]['total_spins']\n",
    "num_runs_0 = datasets[0]['num_runs']\n",
    "\n",
    "# add control data into the dict of datasets\n",
    "control_X = gen_control_data(total_spins_0, num_runs_0)\n",
    "control_reducer = umap.UMAP(**umap_kwargs)\n",
    "control_reducer.fit(control_X)\n",
    "datasets[-1] = {\n",
    "    'label': 'control (coin-flips)',\n",
    "    'num_runs': num_runs_0,\n",
    "    'total_spins': total_spins_0,\n",
    "    'reducer': control_reducer,\n",
    "    'embedding': control_reducer.transform(control_X),\n",
    "    'energies': np.zeros((num_runs, 5)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap_of_data_nonBokeh(data_subdict):\n",
    "    num_runs = data_subdict['num_runs']\n",
    "    label = data_subdict['label']\n",
    "    embedding = data_subdict['embedding']\n",
    "    c = data_subdict['energies'][:, 0]  # range(num_runs)\n",
    "    \n",
    "    \"\"\"for idx in range(5):\n",
    "        calt = data_subdict['energies'][:, idx]\n",
    "        print(calt.shape, np.min(calt), np.max(calt))\n",
    "        plt.hist(calt)\n",
    "        plt.show()\"\"\"\n",
    "    \n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], c=c, cmap='Spectral', s=5)\n",
    "    plt.gca().set_aspect('equal', 'datalim')\n",
    "    #plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "    plt.colorbar()\n",
    "    plt.title('UMAP projection of the %s dataset' % label, fontsize=24)\n",
    "    return\n",
    "\n",
    "#plot_umap_of_data(datasets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bokeh plots multicell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.plot\n",
    "import bokeh.plotting# import figure, output_file, show, output_notebook\n",
    "\n",
    "bokeh.plotting.output_notebook()  # this will render inline jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bokeh_plot(data_subdict, fnamemod=''):\n",
    "    num_runs = data_subdict['num_runs']\n",
    "    label = data_subdict['label']\n",
    "    embedding = data_subdict['embedding']\n",
    "    reducer = data_subdict['reducer']\n",
    "    c = data_subdict['energies'][:, 0]  # range(num_runs)\n",
    "\n",
    "    # setup hoverdata\n",
    "    hover_data = pd.DataFrame({'index': range(num_runs),\n",
    "                               'energy': c,\n",
    "                               'x': embedding[:,0],\n",
    "                               'y': embedding[:,1]})\n",
    "    \n",
    "    # this will load html file in new tab\n",
    "    bokeh.plotting.output_file(\"umap_%s%s.html\" % (label, fnamemod), title=\"umap_%s%s\" % (label, fnamemod))\n",
    "    \n",
    "    #p = umap.plot.points(reducer, labels=c)\n",
    "    #p = umap.plot.interactive(reducer, labels=c, hover_data=hover_data, point_size=2)\n",
    "    \n",
    "    TOOLS = \"pan,wheel_zoom,box_zoom,reset,save,box_select\"\n",
    "    #p = figure(tools=TOOLS, output_backend=\"webgl\")  # called within umap plot interactive probably\n",
    "    \n",
    "    p = umap.plot.interactive(reducer, labels=c, hover_data=hover_data, point_size=4)\n",
    "    p.title.text = 'UMAP projection of the %s dataset' % label\n",
    "    \n",
    "    bokeh.plotting.show(p)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bokeh_plot_alt(data_subdict, fnamemod=''):\n",
    "    num_runs = data_subdict['num_runs']\n",
    "    label = data_subdict['label']\n",
    "    embedding = data_subdict['embedding']\n",
    "    reducer = data_subdict['reducer']\n",
    "    c = data_subdict['energies'][:, 0]  # range(num_runs)\n",
    "    \n",
    "    # OVERLAP HACK\n",
    "    multicell = data_subdict['multicell_template']\n",
    "    simsetup = multicell.simsetup\n",
    "    #print(simsetup['XI'])\n",
    "    overlap_vec_a = np.dot(data_subdict['data'][:, 0:9], simsetup['XI']) / simsetup['XI'].shape[0]\n",
    "    overlap_vec_b = np.dot(data_subdict['data'][:, 9:18], simsetup['XI']) / simsetup['XI'].shape[0]\n",
    "    overlap_vec_c = np.dot(data_subdict['data'][:, 18:27], simsetup['XI']) / simsetup['XI'].shape[0]\n",
    "    overlap_vec_a_string = [np.array2string(a) for a in overlap_vec_a]\n",
    "    overlap_vec_b_string = [np.array2string(a) for a in overlap_vec_b]\n",
    "    overlap_vec_c_string = [np.array2string(a) for a in overlap_vec_c]\n",
    "    print(overlap_vec_a.shape)\n",
    "    proj_choice = overlap_vec_a[:, 0]\n",
    "                  \n",
    "    # setup hoverdata\n",
    "    hover_data = pd.DataFrame({'index': range(num_runs),\n",
    "                               'energy': c,\n",
    "                               'x': embedding[:,0],\n",
    "                               'y': embedding[:,1],\n",
    "                               'cell 0': overlap_vec_a_string,\n",
    "                               'cell 1': overlap_vec_b_string,\n",
    "                               'cell 2': overlap_vec_c_string\n",
    "                              })\n",
    "                               \n",
    "    # this will load html file in new tab\n",
    "    bokeh.plotting.output_file(\"umap_%s%s.html\" % (label, fnamemod), title=\"umap_%s\" % label)\n",
    "    \n",
    "    TOOLS = \"pan,wheel_zoom,box_zoom,reset,save,box_select\"\n",
    "    p = umap.plot.interactive(reducer, labels=proj_choice, hover_data=hover_data, point_size=4)\n",
    "    p.title.text = 'UMAP projection of the %s dataset' % label\n",
    "    \n",
    "    bokeh.plotting.show(p)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gamma_vals)):\n",
    "    bokeh_plot(datasets[i])\n",
    "bokeh_plot(datasets[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding hover pictures (see umap guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subdict = datasets[0]\n",
    "embedding = data_subdict['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddable_image(data):\n",
    "    img_data = 255 - 15 * data.astype(np.uint8)\n",
    "    image = Image.fromarray(img_data, mode='L').resize((64, 64), Image.BICUBIC)\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format='png')\n",
    "    for_encoding = buffer.getvalue()\n",
    "    return 'data:image/png;base64,' + base64.b64encode(for_encoding).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\n",
    "from bokeh.palettes import Spectral10\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_df = pd.DataFrame(embedding, columns=('x', 'y'))\n",
    "digits_df['digit'] = [str(x) for x in data_subdict['energies']]\n",
    "digits_df['image'] = list(map(embeddable_image, digits.images))\n",
    "\n",
    "datasource = ColumnDataSource(digits_df)\n",
    "color_mapping = CategoricalColorMapper(factors=[str(9 - x) for x in digits.target_names],\n",
    "                                       palette=Spectral10)\n",
    "\n",
    "plot_figure = figure(\n",
    "    title='UMAP projection of multicell dataset',\n",
    "    plot_width=600,\n",
    "    plot_height=600,\n",
    "    tools=('pan, wheel_zoom, reset')\n",
    ")\n",
    "\n",
    "plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n",
    "<div>\n",
    "    <div>\n",
    "        <img src='@image' style='float: left; margin: 5px 5px 5px 5px'/>\n",
    "    </div>\n",
    "    <div>\n",
    "        <span style='font-size: 16px; color: #224499'>Digit:</span>\n",
    "        <span style='font-size: 18px'>@digit</span>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "plot_figure.circle(\n",
    "    'x',\n",
    "    'y',\n",
    "    source=datasource,\n",
    "    color=dict(field='digit', transform=color_mapping),\n",
    "    line_alpha=0.6,\n",
    "    fill_alpha=0.6,\n",
    "    size=4\n",
    ")\n",
    "show(plot_figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize arbitrary point in umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_given_multicell(multicell, step_hack, agg_index, outdir):\n",
    "    fpaths = [outdir + os.sep + a for a in\n",
    "              ['agg%d_compOverlap.png' % agg_index,\n",
    "               'agg%d_compProj.png' % agg_index,\n",
    "               'agg%d_ref0_overlap.png' % agg_index]\n",
    "          ]\n",
    "    multicell.step_datadict_update_global(step_hack, fill_to_end=False)\n",
    "    multicell.step_state_visualize(step_hack, fpaths=fpaths)  # visualize\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A: [325, 269, 3918, 5329]\n",
    "# B: [2145, 8616, 6241, 1632]\n",
    "\n",
    "# CHOICES\n",
    "agg_indices = [2145, 8616, 6241, 1632]\n",
    "dataset_label = '0e_10k'\n",
    "outdir = RUNS_FOLDER + os.sep + 'explore' + os.sep + 'B'\n",
    "        \n",
    "# NON-LOOP\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "dataset_index = gamma_vals.index(dataset_label)\n",
    "subdict = datasets[dataset_index]\n",
    "multicell = subdict['multicell_template']\n",
    "\n",
    "for agg_index in agg_indices:\n",
    "    # pull relevant info from subdict\n",
    "    X = subdict['data'][agg_index, :]\n",
    "    step_hack = 0  # TODO care this will break if class has time-varying applied field\n",
    "    multicell.graph_state_arr[:, step_hack] = X[:]\n",
    "    assert np.array_equal(multicell_template.field_applied,\n",
    "                          np.zeros((total_spins, multicell_template.total_steps)))\n",
    "    plot_given_multicell(multicell, step_hack, agg_index, outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting $\\gamma=0$ dual clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_seed = 100\n",
    "spin_low = 5 * 9\n",
    "spin_high = 11 * 9\n",
    "\n",
    "#gamma_vals = ['0e','0.05e', '0.1e', '0.2e', '1e', '2e','20e', '20e_10k']\n",
    "gamma_vals = ['0e_10k']\n",
    "manyruns_datadirs = [RUNS_FOLDER + os.sep + 'multicell_manyruns_gamma%s' % gamma for gamma in gamma_vals]\n",
    "datasets_testing = {i: {'label': 'gamma_%s' % gamma_vals[i]} for i in range(len(gamma_vals))}\n",
    "\n",
    "for idx in range(len(gamma_vals)):\n",
    "    fpath_state = manyruns_datadirs[idx] + os.sep + 'aggregate' + os.sep + 'X_aggregate.npz'\n",
    "    fpath_energy = manyruns_datadirs[idx] + os.sep + 'aggregate' + os.sep + 'X_energy.npz'\n",
    "    fpath_pickle = manyruns_datadirs[idx] + os.sep + 'multicell_template.pkl'\n",
    "    print(fpath_state)\n",
    "    X = np.load(fpath_state)['arr_0'].T               # umap wants transpose\n",
    "    X_energies = np.load(fpath_energy)['arr_0'].T     # umap wants transpose (?)\n",
    "    with open(fpath_pickle, 'rb') as pickle_file:\n",
    "        multicell_template = pickle.load(pickle_file)  # unpickling multicell object\n",
    "    \n",
    "    # LOCAL SUBSAMPLING for testing only\n",
    "    X = X[:, spin_low:spin_high]\n",
    "    print(X.shape)\n",
    "    \n",
    "    # store data and metadata in datasets object\n",
    "    num_runs, total_spins = X.shape\n",
    "    datasets_testing[idx]['data'] = X\n",
    "    datasets_testing[idx]['index'] = list(range(num_runs))\n",
    "    datasets_testing[idx]['energies'] = X_energies\n",
    "    datasets_testing[idx]['num_runs'] = num_runs\n",
    "    datasets_testing[idx]['total_spins'] = total_spins\n",
    "    datasets_testing[idx]['multicell_template'] = multicell_template\n",
    "    \n",
    "    # perform dimension reduction\n",
    "    t1 = time.time()\n",
    "    datasets_testing[idx]['reducer'] = umap.UMAP()                                #random_state=umap_seed)\n",
    "    datasets_testing[idx]['reducer'].fit(X)\n",
    "    datasets_testing[idx]['embedding'] = datasets_testing[idx]['reducer'].transform(X)\n",
    "    print('Time to fit: %.2f sec' % (time.time() - t1))\n",
    "    \n",
    "    # Verify that the result of calling transform is\n",
    "    # idenitical to accessing the embedding_ attribute\n",
    "    assert(np.all(datasets_testing[idx]['embedding'] == datasets_testing[idx]['reducer'].embedding_))\n",
    "    print('embedding.shape:', datasets_testing[idx]['embedding'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdict = datasets_testing[0]\n",
    "bokeh_plot_alt(subdict, fnamemod='_testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = subdict['embedding']\n",
    "print(embedding[7091, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multicell = subdict['multicell_template']\n",
    "simsetup = multicell.simsetup\n",
    "XI = simsetup['XI']\n",
    "print(simsetup['XI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D vis attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://www.kaggle.com/scratchpad/notebook163accf2b7/edit\n",
    "\n",
    "import umap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_3d_mpl(dataset):\n",
    "    \"\"\"\n",
    "    TOO SLOW -- use plotly instead\n",
    "    \"\"\"\n",
    "    embedding = dataset['embedding']\n",
    "    num_runs = embedding.shape[0]\n",
    "    xvec = embedding[:,0]\n",
    "    yvec = embedding[:,1]\n",
    "    zvec = embedding[:,2]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    marker = 'o' # '^'\n",
    "    for idx in range(num_runs):\n",
    "        ax.scatter(xvec[idx], yvec[idx], zvec[idx], marker=marker)\n",
    "\n",
    "    ax.set_xlabel('u1')\n",
    "    ax.set_ylabel('u2')\n",
    "    ax.set_zlabel('u3')\n",
    "    ax.set_title(\"umap_%s\" % dataset['label'])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def umap_plotly_express(data_subdict):\n",
    "    \"\"\"\n",
    "    Supports 2D and 3D embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    num_runs = data_subdict['num_runs']\n",
    "    label = data_subdict['label']\n",
    "    embedding = data_subdict['embedding']\n",
    "    reducer = data_subdict['reducer']\n",
    "    c = data_subdict['energies'][:, 0]  # range(num_runs)\n",
    "\n",
    "    umap_dim = embedding.shape[1]\n",
    "    assert umap_dim in [2,3]\n",
    "    \n",
    "    if umap_dim == 2:\n",
    "        df = pd.DataFrame({'index': range(num_runs),\n",
    "                           'energy': c,\n",
    "                           'x': embedding[:,0],\n",
    "                           'y': embedding[:,1]})\n",
    "\n",
    "        fig = px.scatter(df, x='x', y='y',\n",
    "                         color='energy',\n",
    "                         title='UMAP of %s dataset' % label,\n",
    "                        )\n",
    "\n",
    "    else:\n",
    "        df = pd.DataFrame({'index': range(num_runs),\n",
    "                       'energy': c,\n",
    "                       'x': embedding[:,0],\n",
    "                       'y': embedding[:,1],\n",
    "                       'z': embedding[:,2]})\n",
    "    \n",
    "        fig = px.scatter_3d(df, x='x', y='y', z='z', \n",
    "                            color='energy',\n",
    "                            title='UMAP of %s dataset' % label,\n",
    "                           )\n",
    "    fig.write_html(\"umap_plotly_%s.html\" % label)\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "def umap_plotly_general(dataset):\n",
    "    \"\"\"\"\n",
    "    TODO: implement this more general version of the 'express' code umap_plotly_express() -- see docs\n",
    "    Supports 2D and 3D embeddings\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    # Helix equation\n",
    "    t = np.linspace(0, 10, 50)\n",
    "    x, y, z = np.cos(t), np.sin(t), t\n",
    "\n",
    "    fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z,\n",
    "                                       mode='markers')])\n",
    "    fig.show()\n",
    "\n",
    "#umap_3d_plotly_generic(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gamma_vals)):\n",
    "    umap_plotly_express(datasets[i])\n",
    "umap_plotly_express(datasets[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate umap at $\\gamma=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run path blocks at top first\n",
    "import joblib\n",
    "\n",
    "#gamma0_manyruns = RUNS_FOLDER + os.sep + 'multicell_manyruns' + os.sep + 'gamma2.00_10k'\n",
    "gamma0_manyruns = RUNS_FOLDER + os.sep + 'multicell_manyruns' + os.sep + 'Wrandom0_gamma0.00_10k_fixedorderNot_p3_M100'\n",
    "\n",
    "\n",
    "fpath = gamma0_manyruns + os.sep + 'dimreduce' + os.sep + 'dimreduce_F=umap_dim3_seed100_use01.z'\n",
    "if os.path.isfile(fpath):\n",
    "    print('Exists already, loading: %s' % fpath)\n",
    "    fcontents = joblib.load(fpath)  # just load file if it exists\n",
    "else:\n",
    "    assert 1==2\n",
    "    \n",
    "print(fcontents.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fcontents['energies'][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_to_mirror_labels(X):\n",
    "    # create mirror list -- search for pairs and call them 'A', 'B', any lone points will be called 'Z'\n",
    "    # convention: if two states are MIRROR FLIPS, then\n",
    "    # 'A' is the one for who spin at index 0 is +1\n",
    "    # 'B' is the one for who spin at index 0 is -1\n",
    "    \n",
    "    up_label = 'A'\n",
    "    down_label = 'B'\n",
    "    none_label = 'Z'\n",
    "    spin_to_letter = {1: up_label, -1: down_label}\n",
    "    \n",
    "    print(X.shape)\n",
    "    num_runs = X.shape[0]\n",
    "    mirror_labels = ['' for i in range(num_runs)]\n",
    "    for idx in range(num_runs):\n",
    "        \n",
    "        if idx % 500 == 0:\n",
    "            print('Currently at %d / %d' % (idx, num_runs))\n",
    "        \n",
    "        if mirror_labels[idx] in [up_label, down_label]:\n",
    "            #print(idx, 'CONTINUING')\n",
    "            continue\n",
    "        else:\n",
    "            #print(idx, 'not...')\n",
    "\n",
    "            state_to_compare = X[idx,:]\n",
    "            state_to_compare_flip = -1 * state_to_compare\n",
    "            letter_choice = spin_to_letter[state_to_compare[0]]\n",
    "            letter_choice_flip = spin_to_letter[-1 * state_to_compare[0]]\n",
    "            \n",
    "            bool_mask_cols_equal_to_candidate = np.all(X == state_to_compare, axis=1)\n",
    "            bool_mask_cols_flip_to_candidate = np.all(X == state_to_compare_flip, axis=1)\n",
    "            #print()\n",
    "            \n",
    "            if np.any(bool_mask_cols_flip_to_candidate):\n",
    "                # will have A B pairs\n",
    "                for a, val in enumerate(bool_mask_cols_equal_to_candidate):\n",
    "                    #print(bool_mask_cols_equal_to_candidate)\n",
    "                    #print(val)\n",
    "                    if val:\n",
    "                        #print(idx, 'set %d to %s' % (a, letter_choice))\n",
    "                        mirror_labels[a] = letter_choice\n",
    "                for b, val in enumerate(bool_mask_cols_flip_to_candidate):\n",
    "                    if val:\n",
    "                        #print(idx, 'FLIP set %d to %s' % (b, letter_choice_flip))\n",
    "                        mirror_labels[b] = letter_choice_flip\n",
    "            else:\n",
    "                # will NOT have A B pairs\n",
    "                for j, val in enumerate(bool_mask_cols_equal_to_candidate):\n",
    "                    if val:\n",
    "                        mirror_labels[j] = none_label\n",
    "            \n",
    "    return mirror_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fcontents['mirror_labels'] = X_to_mirror_labels(fcontents['data'])\n",
    "print(fcontents['mirror_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = fcontents['data']\n",
    "X_pm1 = 2*X_data - 1\n",
    "fcontents['mirror_labels'] = X_to_mirror_labels(X_pm1)\n",
    "print(set(fcontents['mirror_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hist = 100\n",
    "num_runs, num_spins = X_pm1.shape\n",
    "\n",
    "state_overlap_sum = np.zeros(num_runs)\n",
    "for idx in range(n_hist):\n",
    "    x0 = X_pm1[idx,:]\n",
    "    state_overlap = np.dot(X_pm1, x0) / float(num_spins)\n",
    "    state_overlap_sum += state_overlap\n",
    "    plt.hist(state_overlap, bins=100, alpha=0.4)\n",
    "    \n",
    "state_overlap_mean = state_overlap_sum / n_hist\n",
    "plt.figure()\n",
    "plt.hist(state_overlap_mean, bins=100, alpha=0.4)\n",
    "plt.title('Mean overlap distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure()\n",
    "#plt.hist(state_overlap_mean, bins=100, alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at overlap of all points with +1 and -1 states**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_spins = X_pm1.shape[1]\n",
    "magnetizations = np.dot(X_pm1, np.ones(total_spins)) / float(total_spins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(magnetizations, bins=100, alpha=0.4)\n",
    "plt.title('magnetizations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K means on umap to get labels for points in each cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_highdim = KMeans(n_clusters=8, random_state=0).fit(X_pm1)\n",
    "kmeans_lowdim = KMeans(n_clusters=8, random_state=0).fit(fcontents['algos']['umap']['embedding'])\n",
    "\n",
    "#kmeans.labels_\n",
    "#kmeans.predict([[0, 0], [12, 3]])\n",
    "#kmeans.cluster_centers_\n",
    "#fit_predict(X, y=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_colour(a):\n",
    "    d = {0: 'blue', 1: 'red', 2:'green', 3:'pruple', 4: 'pink', 5: 'q', 6: 'qq', 7: 'qqq'}\n",
    "    return d[a]\n",
    "\n",
    "fcontents['colours_highdim'] = [label_to_colour(a) for a in kmeans_highdim.labels_]\n",
    "fcontents['colours_lowdim'] = [label_to_colour(a) for a in kmeans_lowdim.labels_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_plotly_clustered(data_subdict):   \n",
    "    \n",
    "    import plotly.express as px\n",
    "\n",
    "    num_runs = data_subdict['num_runs']\n",
    "    label = data_subdict['label']\n",
    "    embedding = data_subdict['algos']['umap']['embedding']\n",
    "    reducer = data_subdict['algos']['umap']['reducer']\n",
    "    #c = data_subdict['energies'][:, 0]  # range(num_runs)\n",
    "    c = data_subdict['colours_lowdim']\n",
    "\n",
    "    umap_dim = embedding.shape[1]\n",
    "    assert umap_dim in [2,3]\n",
    "    \n",
    "    if umap_dim == 2:\n",
    "        df = pd.DataFrame({'index': range(num_runs),\n",
    "                           'c': c,\n",
    "                           'x': embedding[:,0],\n",
    "                           'y': embedding[:,1]})\n",
    "\n",
    "        fig = px.scatter(df, x='x', y='y',\n",
    "                         color='c',\n",
    "                         title='UMAP of %s dataset' % label,\n",
    "                        )\n",
    "\n",
    "    else:\n",
    "        df = pd.DataFrame({'index': range(num_runs),\n",
    "                       'c': c,\n",
    "                       'x': embedding[:,0],\n",
    "                       'y': embedding[:,1],\n",
    "                       'z': embedding[:,2]})\n",
    "    \n",
    "        fig = px.scatter_3d(df, x='x', y='y', z='z', \n",
    "                            color='c',\n",
    "                            title='UMAP of %s dataset' % label,\n",
    "                           )\n",
    "    fig.write_html(\"umap_plotly_%s_clustered.html\" % label)\n",
    "    fig.show()\n",
    "    \n",
    "def plotly_express_embedding_clustered(data_subdict):\n",
    "    \"\"\"\n",
    "    Supports 2D and 3D embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    import plotly.express as px\n",
    "\n",
    "\n",
    "    num_runs = data_subdict['num_runs']\n",
    "    label = data_subdict['label']\n",
    "    dirpath = data_subdict['path'] + os.sep + 'dimreduce'\n",
    "    #c = data_subdict['energies'][:, 0]  # range(num_runs)\n",
    "    c = data_subdict['colours_highdim']\n",
    "\n",
    "    \n",
    "    for key, algodict in data_subdict['algos'].items():\n",
    "        algo = key\n",
    "        reducer = algodict['reducer']\n",
    "        embedding = algodict['embedding']\n",
    "\n",
    "        n_components = embedding.shape[1]\n",
    "        assert n_components in [2, 3]\n",
    "\n",
    "        if n_components == 2:\n",
    "            df = pd.DataFrame({'index': range(num_runs),\n",
    "                               'energy': c,\n",
    "                               'x': embedding[:, 0],\n",
    "                               'y': embedding[:, 1]})\n",
    "\n",
    "            fig = px.scatter(df, x='x', y='y',\n",
    "                             color='energy',\n",
    "                             title='%s of %s dataset' % (algo, label))\n",
    "\n",
    "        else:\n",
    "            df = pd.DataFrame({'index': range(num_runs),\n",
    "                               'energy': c,\n",
    "                               'x': embedding[:, 0],\n",
    "                               'y': embedding[:, 1],\n",
    "                               'z': embedding[:, 2]})\n",
    "\n",
    "            fig = px.scatter_3d(df, x='x', y='y', z='z',\n",
    "                                color='energy',\n",
    "                                title='%s of %s dataset' % (algo, label))\n",
    "\n",
    "        if not os.path.exists(dirpath):\n",
    "            os.makedirs(dirpath)\n",
    "        fig.write_html(dirpath + os.sep + \"%s_plotly_%s_clustered.html\" % (algo, label))\n",
    "        fig.show()\n",
    "    return\n",
    "\n",
    "plotly_express_embedding_clustered(fcontents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare lowdim kmeans labels with statistics in original space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans_lowdim.labels_)\n",
    "\n",
    "blue_indices = [idx for idx, i in enumerate(kmeans_lowdim.labels_) if i == 0]\n",
    "red_indices = [idx for idx, i in enumerate(kmeans_lowdim.labels_) if i == 1]\n",
    "fcontents['data_blue'] = X_pm1[blue_indices, :] \n",
    "fcontents['data_red'] = X_pm1[red_indices, :]\n",
    "\n",
    "print(fcontents['data_blue'].shape)\n",
    "print(fcontents['data_red'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnetizations_blue = np.dot(fcontents['data_blue'], np.ones(total_spins)) / float(total_spins)\n",
    "magnetizations_red = np.dot(fcontents['data_red'], np.ones(total_spins)) / float(total_spins)\n",
    "\n",
    "print(magnetizations_red.shape)\n",
    "print(magnetizations_blue.shape)\n",
    "\n",
    "plt.hist(magnetizations_blue, bins=150, alpha=0.4, color='blue')\n",
    "plt.hist(magnetizations_red, bins=150, alpha=0.4, color='red')\n",
    "plt.title('magnetizations')\n",
    "#plt.ylim(0.,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(magnetizations_blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plate_statistics(plate):\n",
    "    num_spins = plate.shape[0]\n",
    "    #cell_reshape = plate.reshape(100, 9)\n",
    "    magnetization = np.dot(plate, np.ones(num_spins))\n",
    "    return magnetization\n",
    "\n",
    "nn = 700\n",
    "for idx in range(nn, nn+50):\n",
    "    plate = fcontents['data_red'][idx,:]\n",
    "    #plate = plate[0:300]\n",
    "    stats = get_plate_statistics(plate)\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search umap hyperparameters for single clusters**\n",
    "\n",
    "Observe that between nn 2000 and nn 5000 (subsampling the num_runs = 10,000) the split to two clusters occurs\n",
    "\n",
    "For nn 5000, tried 4 random state ints. 3/4 led to disks, 1 led to balls.  \n",
    "\n",
    "Increasing spread from 1.0 to 2.0 causes the split to be present at nn 2000. \n",
    "\n",
    "\n",
    "OTOH, reducing spread to 0.25 causes the clusters to merge for nn 5000.\n",
    "Reducing n_neighbors to 5 (but not 10) from 15 causes same. \n",
    "Increase min_dist to 0.5 (but not 0.2) from 0.1 causes same. \n",
    "\n",
    "\n",
    "MERGING:\n",
    "- reduce spread\n",
    "- reduce n_neighbours\n",
    "- increase min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_metric = 'euclidean'  # also try 'random'\n",
    "default_init = 'spectral'     # also try 'random'\n",
    "default_n_neighbors = 15      # also try 2 to 100\n",
    "default_min_dist = 0.1\n",
    "default_spread = 1.0\n",
    "\n",
    "UMAP_KWARGS = {\n",
    "    'random_state': 0,\n",
    "    'n_components': 3,\n",
    "    'metric': 'euclidean',\n",
    "    'init': 'spectral',\n",
    "    'n_neighbors': 15,\n",
    "    'min_dist': 0.1,\n",
    "    'spread': 1.0,\n",
    "}\n",
    "\n",
    "nn = 10000\n",
    "X = fcontents['data'][0:nn, :]\n",
    "\n",
    "# shuffling order\n",
    "#np.transpose(X)\n",
    "#np.random.shuffle(X)\n",
    "#np.transpose(X)\n",
    "\n",
    "print(X.shape)\n",
    "reducer_alt = umap.UMAP(**UMAP_KWARGS)\n",
    "reducer_alt.fit(X)\n",
    "embedding_alt = reducer_alt.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "c = list(range(nn))\n",
    "\n",
    "df = pd.DataFrame({'index': range(nn),\n",
    "               'c': c,\n",
    "               'x': embedding_alt[:,0],\n",
    "               'y': embedding_alt[:,1],\n",
    "               'z': embedding_alt[:,2]})\n",
    "\n",
    "fig = px.scatter_3d(df, x='x', y='y', z='z', \n",
    "                    color='c',\n",
    "                    title='UMAP of %d dataset' % nn,\n",
    "                   )\n",
    "fig.write_html(fcontents['path'] + os.sep + 'dimreduce' + os.sep + \"umap_plotly_gamma0_nn%d.html\" % nn)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsample = 1000\n",
    "#blue_ss = fcontents['data_blue'][0:subsample,:]\n",
    "#red_ss = fcontents['data_red'][0:subsample,:]\n",
    "\n",
    "blue_ss = fcontents['data_blue'][0:,:]\n",
    "red_ss = fcontents['data_red'][0:,:]\n",
    "\n",
    "X_blue = np.dot(blue_ss, blue_ss.T) / 900\n",
    "X_red = np.dot(red_ss, red_ss.T) / 900\n",
    "X_cross = np.dot(blue_ss, red_ss.T) / 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Blue: shape', X_blue.shape)\n",
    "print(np.min(X_blue.flatten()), np.max(X_blue.flatten()))\n",
    "print('Red: shape', X_red.shape)\n",
    "print(np.min(X_red.flatten()), np.max(X_red.flatten()))\n",
    "print('Cross: shape', X_cross.shape)\n",
    "print(np.min(X_cross.flatten()), np.max(X_cross.flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure()\n",
    "bins_cross = 185\n",
    "density = True\n",
    "\n",
    "_, bins, _ = plt.hist(X_cross.flatten(), bins=bins_cross, alpha=0.4, color='k', label='cross', density=density)\n",
    "plt.hist(X_blue.flatten(), bins=bins, alpha=0.4, color='blue', label='blue', density=density)\n",
    "plt.hist(X_red.flatten(), bins=bins, alpha=0.4, color='red', label='red', density=density)\n",
    "plt.legend()\n",
    "plt.title('Similarity element distribution')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins2, _ = plt.hist(X_blue.flatten(), bins=160, alpha=0.4, color='blue', label='blue', density=density)\n",
    "plt.ylim()\n",
    "plt.show()\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistics of each cell type in the two clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spins_to_types(data_arr):\n",
    "    \n",
    "    A = (1,1,1, -1,-1,-1, -1,-1,-1)\n",
    "    B = (-1,-1,-1, 1,1,1, -1,-1,-1)\n",
    "    C = (-1,-1,-1, -1,-1,-1, 1,1,1)\n",
    "    S = (1,1,1, 1,1,1, 1,1,1)\n",
    "    \n",
    "    mapping = {\n",
    "        A: 1,\n",
    "        B: 2,\n",
    "        C: 3,\n",
    "        S: 4,\n",
    "        tuple([-a for a in A]): -1,\n",
    "        tuple([-a for a in B]): -2,\n",
    "        tuple([-a for a in C]): -3,\n",
    "        tuple([-a for a in S]): -4\n",
    "    }\n",
    "    \n",
    "    nruns, nspins = data_arr.shape\n",
    "    ncells = int(nspins / 9)\n",
    "    celltypes_arr = np.zeros((nruns, ncells))\n",
    "    for i in range(nruns):\n",
    "        datarow = data_arr[i,:]\n",
    "        celltypes_arr[i,:] = np.array([mapping[tuple(datarow[a*9:(a+1)*9])] for a in range(ncells)])\n",
    "    \n",
    "    return celltypes_arr\n",
    "\n",
    "\n",
    "def spins_to_mag(data_arr):\n",
    "    \n",
    "    A = (1,1,1, -1,-1,-1, -1,-1,-1)\n",
    "    B = (-1,-1,-1, 1,1,1, -1,-1,-1)\n",
    "    C = (-1,-1,-1, -1,-1,-1, 1,1,1)\n",
    "    S = (1,1,1, 1,1,1, 1,1,1)\n",
    "    \n",
    "    q = float(1/3)\n",
    "    \n",
    "    mapping = {\n",
    "        A: -q,\n",
    "        B: -q,\n",
    "        C: -q,\n",
    "        S: 1,\n",
    "        tuple([-a for a in A]): q,\n",
    "        tuple([-a for a in B]): q,\n",
    "        tuple([-a for a in C]): q,\n",
    "        tuple([-a for a in S]): -1\n",
    "    }\n",
    "    \n",
    "    nruns, nspins = data_arr.shape\n",
    "    ncells = int(nspins / 9)\n",
    "    celltypes_arr = np.zeros((nruns, ncells))\n",
    "    for i in range(nruns):\n",
    "        datarow = data_arr[i,:]\n",
    "        celltypes_arr[i,:] = np.array([mapping[tuple(datarow[a*9:(a+1)*9])] for a in range(ncells)])\n",
    "    \n",
    "    return celltypes_arr\n",
    "\n",
    "def spins_to_magsum(data_arr):\n",
    "    \n",
    "    A = (1,1,1, -1,-1,-1, -1,-1,-1)\n",
    "    B = (-1,-1,-1, 1,1,1, -1,-1,-1)\n",
    "    C = (-1,-1,-1, -1,-1,-1, 1,1,1)\n",
    "    S = (1,1,1, 1,1,1, 1,1,1)\n",
    "    \n",
    "    q = float(1/3)\n",
    "    \n",
    "    mapping = {\n",
    "        A: -q,\n",
    "        B: -q,\n",
    "        C: -q,\n",
    "        S: 1,\n",
    "        tuple([-a for a in A]): q,\n",
    "        tuple([-a for a in B]): q,\n",
    "        tuple([-a for a in C]): q,\n",
    "        tuple([-a for a in S]): -1\n",
    "    }\n",
    "    \n",
    "    nruns, nspins = data_arr.shape\n",
    "    ncells = int(nspins / 9)\n",
    "    celltypes_arr = np.zeros(nruns)\n",
    "    for i in range(nruns):\n",
    "        datarow = data_arr[i,:]\n",
    "        celltypes_arr[i] = np.sum([mapping[tuple(datarow[a*9:(a+1)*9])] for a in range(ncells)])\n",
    "    \n",
    "    return celltypes_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celltypes_blue = spins_to_types(fcontents['data_blue'])\n",
    "celltypes_red = spins_to_types(fcontents['data_red'])\n",
    "mags_blue = spins_to_mag(fcontents['data_blue'])\n",
    "mags_red = spins_to_mag(fcontents['data_red'])\n",
    "mags_blue_sum = spins_to_magsum(fcontents['data_blue'])\n",
    "mags_red_sum = spins_to_magsum(fcontents['data_red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_explicit = [a+0.5 for a in range(-5,5)]\n",
    "print(bins_explicit)\n",
    "\n",
    "plt.hist(celltypes_blue.flatten(), bins=bins_explicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(celltypes_red.flatten(), bins=bins_explicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_explicit_mag = [-1.25,-0.75,-0.6,-0.1,0.1,0.6,0.75,1.25]\n",
    "print(bins_explicit_mag)\n",
    "plt.hist(mags_blue.flatten(), bins=bins_explicit_mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mags_red.flatten(), bins=bins_explicit_mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bins_explicit_mag = [-1.25,-0.75,-0.6,-0.1,0.1,0.6,0.75,1.25]\n",
    "#print(bins_explicit_mag)\n",
    "plt.hist(mags_blue_sum, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bins_explicit_mag = [-1.25,-0.75,-0.6,-0.1,0.1,0.6,0.75,1.25]\n",
    "#print(bins_explicit_mag)\n",
    "plt.hist(mags_red_sum, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celltypes_all = spins_to_types(X_pm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(celltypes_all.flatten(), bins=bins_explicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for idx in range(10):\n",
    "    #plt.hist(celltypes_red[idx,:], bins=bins_explicit, alpha=0.1)\n",
    "np.bincount(celltypes_red.flatten().astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicit celltype counts by run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, c = np.unique(celltypes_blue.flatten().astype(int), return_counts=True)\n",
    "print(np.stack([u, c]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_count_arr(celltypes_arr):\n",
    "\n",
    "    count_arr = np.zeros((8, celltypes_arr.shape[0]), dtype=int)\n",
    "\n",
    "    u0 = [-4,-3,-2,-1,1,2,3,4]\n",
    "    mapping_u_to_loc = {\n",
    "        -4:0,\n",
    "        -3:1,\n",
    "        -2:2,\n",
    "        -1:3,\n",
    "         1:4,\n",
    "         2:5,\n",
    "         3:6,\n",
    "         4:7,\n",
    "    }\n",
    "\n",
    "    for idx in range(celltypes_arr.shape[0]):\n",
    "        uvals, cvals = np.unique(celltypes_arr[idx,:].astype(int), return_counts=True)\n",
    "        for j, u in enumerate(uvals):        \n",
    "            u_to_loc = mapping_u_to_loc[u]\n",
    "            count_arr[u_to_loc, idx] = cvals[j]\n",
    "\n",
    "    print(np.mean(count_arr, axis=1))\n",
    "    return count_arr\n",
    "\n",
    "red_count_arr = gen_count_arr(celltypes_red)\n",
    "blue_count_arr = gen_count_arr(celltypes_blue)\n",
    "all_count_arr = gen_count_arr(celltypes_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(red_count_arr[0,:], bins=np.arange(-0.5,101.5), color='purple', label='-S', alpha=0.5)\n",
    "plt.hist(red_count_arr[7,:], bins=np.arange(-0.5,101.5), color='green', label='+S', alpha=0.5)\n",
    "#plt.hist(red_count_arr[3,:], bins=np.arange(-0.5,101.5), color='orange', label='+A', alpha=0.5)\n",
    "#plt.hist(red_count_arr[2,:], bins=np.arange(-0.5,101.5), color='pink', label='-A', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title('For RED: freq of -S vs +S cell types')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(blue_count_arr[0,:], bins=np.arange(-0.5,101.5), color='purple', label='-S', alpha=0.5)\n",
    "plt.hist(blue_count_arr[7,:], bins=np.arange(-0.5,101.5), color='green', label='+S', alpha=0.5)\n",
    "#plt.hist(red_count_arr[3,:], bins=np.arange(-0.5,101.5), color='orange', label='+A', alpha=0.5)\n",
    "#plt.hist(red_count_arr[2,:], bins=np.arange(-0.5,101.5), color='pink', label='-A', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title('For BLUE: freq of -S vs +S cell types')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(red_count_arr[3,:], bins=np.arange(-0.5,101.5), color='purple', label='-A', alpha=0.5)\n",
    "plt.hist(red_count_arr[4,:], bins=np.arange(-0.5,101.5), color='green', label='+A', alpha=0.5)\n",
    "#plt.hist(red_count_arr[3,:], bins=np.arange(-0.5,101.5), color='orange', label='+A', alpha=0.5)\n",
    "#plt.hist(red_count_arr[2,:], bins=np.arange(-0.5,101.5), color='pink', label='-A', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title('For RED: freq of -A vs +A cell types')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(blue_count_arr[3,:], bins=np.arange(-0.5,101.5), color='purple', label='-A', alpha=0.5)\n",
    "plt.hist(blue_count_arr[4,:], bins=np.arange(-0.5,101.5), color='green', label='+A', alpha=0.5)\n",
    "#plt.hist(red_count_arr[3,:], bins=np.arange(-0.5,101.5), color='orange', label='+A', alpha=0.5)\n",
    "#plt.hist(red_count_arr[2,:], bins=np.arange(-0.5,101.5), color='pink', label='-A', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title('For BLUE: freq of -A vs +A cell types')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(blue_count_arr[2,:], bins=np.arange(-0.5,101.5), color='purple', label='-B', alpha=0.5)\n",
    "plt.hist(blue_count_arr[5,:], bins=np.arange(-0.5,101.5), color='green', label='+B', alpha=0.5)\n",
    "#plt.hist(red_count_arr[3,:], bins=np.arange(-0.5,101.5), color='orange', label='+A', alpha=0.5)\n",
    "#plt.hist(red_count_arr[2,:], bins=np.arange(-0.5,101.5), color='pink', label='-A', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title('For BLUE: freq of -B vs +B cell types')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(blue_count_arr[1,:], bins=np.arange(-0.5,101.5), color='purple', label='-C', alpha=0.5)\n",
    "plt.hist(blue_count_arr[6,:], bins=np.arange(-0.5,101.5), color='green', label='+C', alpha=0.5)\n",
    "#plt.hist(red_count_arr[3,:], bins=np.arange(-0.5,101.5), color='orange', label='+A', alpha=0.5)\n",
    "#plt.hist(red_count_arr[2,:], bins=np.arange(-0.5,101.5), color='pink', label='-A', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title('For BLUE: freq of -C vs +C cell types')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(red_count_arr[0,:] - red_count_arr[7,:], bins=59, color='grey', alpha=0.7)\n",
    "#plt.hist(red_count_arr[3,:], bins=np.arange(-0.5,101.5), color='orange', label='+A', alpha=0.5)\n",
    "#plt.hist(red_count_arr[2,:], bins=np.arange(-0.5,101.5), color='pink', label='-A', alpha=0.5)\n",
    "plt.axvline(0, linewidth=4)\n",
    "plt.legend()\n",
    "plt.title('For RED: freq of difference \"-S minus +S\" cell types')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(blue_count_arr[0,:] - blue_count_arr[7,:], bins=59, color='grey', alpha=0.7)\n",
    "#plt.hist(red_count_arr[3,:], bins=np.arange(-0.5,101.5), color='orange', label='+A', alpha=0.5)\n",
    "#plt.hist(red_count_arr[2,:], bins=np.arange(-0.5,101.5), color='pink', label='-A', alpha=0.5)\n",
    "plt.axvline(0, linewidth=4)\n",
    "plt.legend()\n",
    "plt.title('For BLUE: freq of difference \"-S minus +S\" cell types')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_count_arr[0,:] - all_count_arr[7,:], bins=46, color='grey', alpha=0.7)\n",
    "#plt.hist(red_count_arr[3,:], bins=np.arange(-0.5,101.5), color='orange', label='+A', alpha=0.5)\n",
    "#plt.hist(red_count_arr[2,:], bins=np.arange(-0.5,101.5), color='pink', label='-A', alpha=0.5)\n",
    "plt.axvline(0, linewidth=4)\n",
    "plt.legend()\n",
    "plt.title('For BLUE: freq of difference \"-S minus +S\" cell types')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing UNIQUE flag for umap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsupervised_helper import *\n",
    "\n",
    "build_dimreduce_dicts = True\n",
    "add_control_data = False\n",
    "vis_all = True\n",
    "pca_assess = False\n",
    "nsubsample = None  # None or an int\n",
    "\n",
    "# Step 0) which 'manyruns' dirs to work with\n",
    "#gamma_list = [0.0, 0.05, 0.1, 0.2, 1.0, 2.0, 20.0]\n",
    "gamma_list = [20.0]\n",
    "#gamma_list = [20.0]\n",
    "#manyruns_dirnames = ['Wrandom1_gamma%.2f_10k_fixedorder_ferro' % a for a in gamma_list]\n",
    "#manyruns_dirnames = ['Wrandom0_gamma%.2f_10k_p3_M100' % a for a in gamma_list]\n",
    "manyruns_dirnames = ['Wrandom1_gamma%.2f_10k_fixedorder_p3_M100' % a for a in gamma_list]\n",
    "\n",
    "manyruns_paths = [RUNS_FOLDER + os.sep + 'multicell_manyruns' + os.sep + dirname\n",
    "                  for dirname in manyruns_dirnames]\n",
    "\n",
    "# Step 1) umap (or other dim reduction) kwargs\n",
    "if any([build_dimreduce_dicts, add_control_data, vis_all, pca_assess]):\n",
    "    for n_components in [2, 3]:\n",
    "        #n_components = 3\n",
    "        pca_kwargs = PCA_KWARGS.copy()\n",
    "        pca_kwargs['n_components'] = n_components  # TODO don't need to spec 'live', can embed later?\n",
    "        umap_kwargs = UMAP_KWARGS.copy()\n",
    "        umap_kwargs['n_components'] = n_components  # TODO don't need to spec 'live', can embed later?\n",
    "        tsne_kwargs = TSNE_KWARGS.copy()\n",
    "        tsne_kwargs['n_components'] = n_components  # TODO don't need to spec 'live', can embed later?\n",
    "        # modify pca settings\n",
    "        # modify umap settings\n",
    "        umap_kwargs['unique'] = True\n",
    "        #umap_kwargs['n_neighbors'] = 100\n",
    "        #umap_kwargs['min_dist'] = 0.1\n",
    "        #umap_kwargs['spread'] = 1.0\n",
    "        #umap_kwargs['metric'] = 'euclidean'\n",
    "        # modify tsne settings\n",
    "        tsne_kwargs['perplexity'] = 100\n",
    "\n",
    "        # Modify filename suffix for dimreduce pkl and plots\n",
    "        fmod = '_F=' + '+'.join(REDUCERS_TO_USE)\n",
    "        fmod += '_dim%d_seed%d' % (umap_kwargs['n_components'], umap_kwargs['random_state'])\n",
    "        if nsubsample is not None:\n",
    "            fmod += '_nn%d' % nsubsample\n",
    "        if 'umap' in REDUCERS_TO_USE:\n",
    "            if umap_kwargs['metric'] != 'euclidean':\n",
    "                fmod += '_%s' % umap_kwargs['metric']\n",
    "            if umap_kwargs['init'] != 'spectral':\n",
    "                fmod += '_%s' % umap_kwargs['init']\n",
    "            if umap_kwargs['n_neighbors'] != 15:\n",
    "                fmod += '_nbor%d' % umap_kwargs['n_neighbors']\n",
    "            if umap_kwargs['min_dist'] != 0.1:\n",
    "                fmod += '_dist%.2f' % umap_kwargs['min_dist']\n",
    "            if umap_kwargs['spread'] != 1.0:\n",
    "                fmod += '_spread%.2f' % umap_kwargs['spread']\n",
    "            if umap_kwargs['unique']:\n",
    "                fmod += '_unique'\n",
    "        if 'tsne' in REDUCERS_TO_USE:\n",
    "            if tsne_kwargs['perplexity'] != 30.0:\n",
    "                fmod += '_perplex%.2f' % tsne_kwargs['perplexity']\n",
    "\n",
    "        # Step 2) make/load data\n",
    "        datasets = {i: {'label': manyruns_dirnames[i],\n",
    "                        'path': manyruns_paths[i]}\n",
    "                    for i in range(len(manyruns_dirnames))}\n",
    "\n",
    "        for idx in range(len(manyruns_dirnames)):\n",
    "            fpath = manyruns_paths[idx] + os.sep + 'dimreduce' + os.sep + 'dimreduce%s.z' % fmod\n",
    "            if os.path.isfile(fpath):\n",
    "                print('Exists already, loading: %s' % fpath)\n",
    "                fcontents = joblib.load(fpath)  # just load file if it exists\n",
    "                datasets[idx] = fcontents\n",
    "            else:\n",
    "                print('Dim. reduction on manyruns: %s' % manyruns_dirnames[idx])\n",
    "                datasets[idx] = make_dimreduce_object(\n",
    "                    datasets[idx], nsubsample=nsubsample, flag_control=False,\n",
    "                    umap_kwargs=umap_kwargs, tsne_kwargs=tsne_kwargs, pca_kwargs=pca_kwargs)\n",
    "                save_dimreduce_object(datasets[idx], fpath)  # save to file (joblib)\n",
    "\n",
    "        if add_control_data:\n",
    "            print('adding control data...')\n",
    "            total_spins_0 = datasets[0]['total_spins']\n",
    "            num_runs_0 = datasets[0]['num_runs']\n",
    "\n",
    "            # add control data into the dict of datasets\n",
    "            control_X = generate_control_data(total_spins_0, num_runs_0)\n",
    "            control_folder = RUNS_FOLDER + os.sep + 'multicell_manyruns' + os.sep + 'control'\n",
    "            control_fpath = control_folder + os.sep + \\\n",
    "                            'dimreduce' + os.sep + 'dimreduce%s.z' % fmod\n",
    "\n",
    "            datasets[-1] = {\n",
    "                'data': control_X,\n",
    "                'label': 'control (coin-flips)',\n",
    "                'num_runs': num_runs_0,\n",
    "                'total_spins': total_spins_0,\n",
    "                'energies': np.zeros((num_runs_0, 5)),\n",
    "                'path': control_folder\n",
    "            }\n",
    "            datasets[-1] = make_dimreduce_object(\n",
    "                datasets[-1], flag_control=True, nsubsample=nsubsample,\n",
    "                umap_kwargs=umap_kwargs, tsne_kwargs=tsne_kwargs, pca_kwargs=pca_kwargs)\n",
    "            save_dimreduce_object(datasets[-1], control_fpath)  # save to file (joblib)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing plotly surface plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Read data from a csv\n",
    "z_data = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/api_docs/mt_bruno_elevation.csv')\n",
    "\n",
    "#print(z_data['x'])\n",
    "data_top = z_data.head()  \n",
    "print(data_top)\n",
    "print(z_data.shape)\n",
    "print(z_data.values.shape)\n",
    "print(type(z_data.values))\n",
    "a = z_data.values\n",
    "a.reshape(1, len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Surface(z=z_data.values)])\n",
    "\n",
    "fig.update_layout(title='Mt Bruno Elevation', autosize=False,\n",
    "                  width=500, height=500,\n",
    "                  margin=dict(l=65, r=50, b=65, t=90))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aligned umap testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import umap\n",
    "#import umap.plot\n",
    "import umap.utils as utils\n",
    "import umap.aligned_umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = sklearn.datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_dict = {i:i for i in range(digits.data.shape[0])}\n",
    "constant_relations = [constant_dict for i in range(9)]\n",
    "\n",
    "neighbors_mapper = umap.AlignedUMAP(\n",
    "    n_neighbors=[3,4,5,7,11,16,22,29,37,45,54],\n",
    "    min_dist=[0.01,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45],\n",
    "    alignment_window_size=2,\n",
    "    alignment_regularisation=1e-3,\n",
    ").fit(\n",
    "    [digits.data for i in range(10)], relations=constant_relations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embeddings = len(neighbors_mapper.embeddings_)\n",
    "es = neighbors_mapper.embeddings_\n",
    "embedding_df = pd.DataFrame(np.vstack(es), columns=('x', 'y'))\n",
    "embedding_df['z'] = np.repeat(np.linspace(0, 1.0, n_embeddings), es[0].shape[0])\n",
    "embedding_df['id'] = np.tile(np.arange(es[0].shape[0]), n_embeddings)\n",
    "embedding_df['digit'] = np.tile(digits.target, n_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.interpolate\n",
    "\n",
    "fx = scipy.interpolate.interp1d(\n",
    "    embedding_df.z[embedding_df.id == 0], embedding_df.x.values.reshape(n_embeddings, digits.data.shape[0]).T, kind=\"cubic\"\n",
    ")\n",
    "fy = scipy.interpolate.interp1d(\n",
    "    embedding_df.z[embedding_df.id == 0], embedding_df.y.values.reshape(n_embeddings, digits.data.shape[0]).T, kind=\"cubic\"\n",
    ")\n",
    "z = np.linspace(0, 1.0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = px.colors.diverging.Spectral\n",
    "interpolated_traces = [fx(z), fy(z)]\n",
    "traces = [\n",
    "    go.Scatter3d(\n",
    "        x=interpolated_traces[0][i],\n",
    "        y=interpolated_traces[1][i],\n",
    "        z=z*3.0,\n",
    "        mode=\"lines\",\n",
    "        line=dict(\n",
    "            color=palette[digits.target[i]],\n",
    "            width=3.0\n",
    "        ),\n",
    "        opacity=1.0,\n",
    "    )\n",
    "    for i in range(digits.data.shape[0])\n",
    "]\n",
    "fig = go.Figure(data=traces)\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=700,\n",
    "    autosize=False,\n",
    "    showlegend=False,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html('zzz.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate way, top of\n",
    "# https://umap-learn.readthedocs.io/en/latest/aligned_umap_basic_usage.html\n",
    "\n",
    "ordered_digits = digits.data[np.argsort(digits.data.sum(axis=1))]\n",
    "ordered_target = digits.target[np.argsort(digits.data.sum(axis=1))]\n",
    "plt.matshow(ordered_digits[-1].reshape((8,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = [ordered_digits[150 * i:min(ordered_digits.shape[0], 150 * i + 400)] for i in range(10)]\n",
    "relation_dict = {i+150:i for i in range(400-150)}\n",
    "relation_dicts = [relation_dict.copy() for i in range(len(slices) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(slices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(slices[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_mapper = umap.AlignedUMAP().fit(slices, relations=relation_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def axis_bounds(embedding):\n",
    "    left, right = embedding.T[0].min(), embedding.T[0].max()\n",
    "    bottom, top = embedding.T[1].min(), embedding.T[1].max()\n",
    "    adj_h, adj_v = (right - left) * 0.1, (top - bottom) * 0.1\n",
    "    return [left - adj_h, right + adj_h, bottom - adj_v, top + adj_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5,2, figsize=(10, 20))\n",
    "ax_bound = axis_bounds(np.vstack(aligned_mapper.embeddings_))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    current_target = ordered_target[150 * i:min(ordered_target.shape[0], 150 * i + 400)]\n",
    "    ax.scatter(*aligned_mapper.embeddings_[i].T, s=2, c=current_target, cmap=\"Spectral\")\n",
    "    ax.axis(ax_bound)\n",
    "    ax.set(xticks=[], yticks=[])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Morphing the above example to our celltype data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDUCER_SEED = 100\n",
    "REDUCER_COMPONENTS = 2\n",
    "UMAP_KWARGS = {\n",
    "    'random_state': REDUCER_SEED,\n",
    "    'n_components': REDUCER_COMPONENTS,\n",
    "    'metric': 'euclidean',\n",
    "    'init': 'spectral',\n",
    "    'unique': False,\n",
    "    'n_neighbors': 15,\n",
    "    'min_dist': 0.1,\n",
    "    'spread': 1.0,\n",
    "}\n",
    "\n",
    "use_01 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Step 0) which 'manyruns' dirs to work with\n",
    "    # gamma_list = [0.0, 0.05, 0.1, 0.2, 1.0, 2.0, 20.0]\n",
    "    gamma_list = [0.2]\n",
    "    # step_list = [0.0, 10.0]  # list of [None] or list of steps\n",
    "    step_list = [0, 5, 10,15,20,25,29]\n",
    "\n",
    "    # gamma_list = [2.0, 20.0]\n",
    "    # manyruns_dirnames = ['Wrandom1_gamma%.2f_10k_fixedorder_ferro' % a for a in gamma_list]\n",
    "    # manyruns_dirnames = ['Wrandom0_gamma%.2f_10k_p3_M100' % a for a in gamma_list]\n",
    "    # manyruns_dirnames = ['Wrandom0_gamma%.2f_10k_fixedorder_p3_M100' % a for a in gamma_list]\n",
    "    # manyruns_dirnames = ['Wrandom1_gamma%.2f_10k_fixedorder_p3_M100' % a for a in gamma_list]\n",
    "\n",
    "    manyruns_dirnames = ['beta2.00_Wrandom0_gamma%.2f_1k_p3_M100' % a for a in\n",
    "                         gamma_list]\n",
    "\n",
    "    manyruns_paths = [RUNS_FOLDER + os.sep + 'multicell_manyruns' + os.sep + dirname\n",
    "                      for dirname in manyruns_dirnames]\n",
    "\n",
    "    # Step 1) umap (or other dim reduction) kwargs\n",
    "    n_components = 2\n",
    "    nn = 1000\n",
    "    X_multi = np.zeros((len(step_list), nn, 900))\n",
    "    for j, step in enumerate(step_list):\n",
    "        umap_kwargs = UMAP_KWARGS.copy()\n",
    "        umap_kwargs[\n",
    "            'n_components'] = n_components  # TODO don't need to spec 'live', can embed later?\n",
    "        # modify umap settings\n",
    "        # umap_kwargs['unique'] = True\n",
    "        # umap_kwargs['n_neighbors'] = 100\n",
    "        # umap_kwargs['min_dist'] = 0.\n",
    "        # umap_kwargs['spread'] = 1.0\n",
    "        # umap_kwargs['metric'] = 'euclidean'\n",
    "\n",
    "        # Step 2) make/load data\n",
    "        # ...\n",
    "        \n",
    "        smod = '_%d' % step\n",
    "        \n",
    "        manyruns_path = manyruns_paths[0]  # TODO\n",
    "        agg_dir = manyruns_path + os.sep + 'aggregate'\n",
    "        fpath_state = agg_dir + os.sep + 'X_aggregate%s.npz' % smod\n",
    "        fpath_energy = agg_dir + os.sep + 'X_energy%s.npz' % smod\n",
    "        fpath_pickle = manyruns_path + os.sep + 'multicell_template.pkl'\n",
    "        print(fpath_state)\n",
    "        \n",
    "        X = np.load(fpath_state)['arr_0'].T  # umap wants transpose\n",
    "        X_energies = np.load(fpath_energy)['arr_0'].T  # umap wants transpose (?)\n",
    "        with open(fpath_pickle, 'rb') as pickle_file:\n",
    "            multicell_template = pickle.load(pickle_file)  # unpickling multicell object\n",
    "        \n",
    "        if use_01:\n",
    "            X = (1 + X) / 2.0\n",
    "            X = X.astype(int)\n",
    "        \n",
    "        X_multi[j,:,:] = X[0:nn, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP aligned needs a relationdict for the 'time varying' dataset\n",
    "# our relation is that each traj maps to itself (in time) -- constant relation\n",
    "\n",
    "constant_dict = {i:i for i in range(900)}\n",
    "constant_relations = [constant_dict for i in range(len(step_list)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_multi_as_list = [X_multi[i,:,:] for i in range(X_multi.shape[0])]\n",
    "aligned_mapper = umap.AlignedUMAP().fit(X_multi, relations=constant_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def axis_bounds(embedding):\n",
    "    left, right = embedding.T[0].min(), embedding.T[0].max()\n",
    "    bottom, top = embedding.T[1].min(), embedding.T[1].max()\n",
    "    adj_h, adj_v = (right - left) * 0.1, (top - bottom) * 0.1\n",
    "    return [left - adj_h, right + adj_h, bottom - adj_v, top + adj_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5,2, figsize=(10, 20))\n",
    "ax_bound = axis_bounds(np.vstack(aligned_mapper.embeddings_))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    print(i)\n",
    "    #current_target = ordered_target[150 * i:min(ordered_target.shape[0], 150 * i + 400)]\n",
    "    ax.scatter(*aligned_mapper.embeddings_[i].T, s=2, cmap=\"Spectral\")\n",
    "    #ax.scatter(*aligned_mapper.embeddings_[i].T, s=2, cmap=\"Spectral_r\")\n",
    "    ax.axis(ax_bound)\n",
    "    ax.set(xticks=[], yticks=[])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Allign UMAP but for Gamma (not step)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDUCER_SEED = 100\n",
    "REDUCER_COMPONENTS = 2\n",
    "UMAP_KWARGS = {\n",
    "    'random_state': REDUCER_SEED,\n",
    "    'n_components': REDUCER_COMPONENTS,\n",
    "    'metric': 'euclidean',\n",
    "    'init': 'spectral',\n",
    "    'unique': False,\n",
    "    'n_neighbors': 15,\n",
    "    'min_dist': 0.1,\n",
    "    'spread': 1.0,\n",
    "}\n",
    "\n",
    "use_01 = True\n",
    "nn = 4000  # runs with 1000, crash with 5000, 10000 -- try to restrict to more int gammas maybe\n",
    "kk = 900   # debug: subsample multicell spins to avoid memory issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0) which 'manyruns' dirs to work with\n",
    "# gamma_list = [0.0, 0.05, 0.1, 0.2, 1.0, 2.0, 20.0]\n",
    "\n",
    "#gamma_list = [0.0, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.15, 0.20, 0.4, 0.6, 0.8, 0.9, 1.0, 20.0]\n",
    "\n",
    "#gamma_list = [0.0, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.15, 0.20, 0.4, 0.6, 0.8, 0.9, 1.0, 20.0]\n",
    "#gamma_list = [0.0, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.15, 0.20, 0.4, 0.6, 0.8, 1.0, 20.0]\n",
    "gamma_list = [0.0, 0.02, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.15, 0.2, 0.4, 0.6, 0.8, 1.0, 2.0, 20.0]\n",
    "#gamma_list = gamma_list[::-1]\n",
    "\n",
    "#gamma_list = [0.0, 0.05]  # , 0.06]# , 0.07] # , 0.08, 0.09, 0.10] #, 0.15, 0.20, 0.4, 0.6, 0.8, 0.9, 1.0, 20.0]\n",
    "\n",
    "# gamma_list = [2.0, 20.0]\n",
    "# manyruns_dirnames = ['Wrandom1_gamma%.2f_10k_fixedorder_ferro' % a for a in gamma_list]\n",
    "# manyruns_dirnames = ['Wrandom0_gamma%.2f_10k_p3_M100' % a for a in gamma_list]\n",
    "# manyruns_dirnames = ['Wrandom0_gamma%.2f_10k_fixedorder_p3_M100' % a for a in gamma_list]\n",
    "# manyruns_dirnames = ['Wrandom1_gamma%.2f_10k_fixedorder_p3_M100' % a for a in gamma_list]\n",
    "\n",
    "#manyruns_dirnames = ['Wrandom0_gamma%.2f_10k_periodic_fixedorderV3_p3_M100' % a for a in\n",
    "#                     gamma_list]\n",
    "manyruns_dirnames = ['Wmaze15_gamma%.2f_10k_p3_M100' % a for a in\n",
    "                     gamma_list]\n",
    "\n",
    "manyruns_paths = [RUNS_FOLDER + os.sep + 'multicell_manyruns' + os.sep + dirname\n",
    "                  for dirname in manyruns_dirnames]\n",
    "\n",
    "# Step 1) umap (or other dim reduction) kwargs\n",
    "n_components = 2\n",
    "\n",
    "X_multi = np.zeros((len(gamma_list), nn, kk), dtype=int)\n",
    "for j, manyruns_path in enumerate(manyruns_paths):\n",
    "\n",
    "    gamma_val = gamma_list[j]\n",
    "\n",
    "    umap_kwargs = UMAP_KWARGS.copy()\n",
    "    umap_kwargs['n_components'] = n_components  # TODO don't need to spec 'live', can embed later?\n",
    "    # modify umap settings\n",
    "    # umap_kwargs['unique'] = True\n",
    "    # umap_kwargs['n_neighbors'] = 100\n",
    "    umap_kwargs['min_dist'] = 0.25\n",
    "    # umap_kwargs['spread'] = 1.0\n",
    "    # umap_kwargs['metric'] = 'euclidean'\n",
    "\n",
    "    # Step 2) make/load data\n",
    "    # ...\n",
    "\n",
    "    smod = '_last'\n",
    "\n",
    "    agg_dir = manyruns_path + os.sep + 'aggregate'\n",
    "    fpath_state = agg_dir + os.sep + 'X_aggregate%s.npz' % smod\n",
    "    fpath_energy = agg_dir + os.sep + 'X_energy%s.npz' % smod\n",
    "    fpath_pickle = manyruns_path + os.sep + 'multicell_template.pkl'\n",
    "\n",
    "    X = np.load(fpath_state)['arr_0'].T  # umap wants transpose\n",
    "    X_energies = np.load(fpath_energy)['arr_0'].T  # umap wants transpose (?)\n",
    "    with open(fpath_pickle, 'rb') as pickle_file:\n",
    "        multicell_template = pickle.load(pickle_file)  # unpickling multicell object\n",
    "\n",
    "    if use_01:\n",
    "        X = (1 + X) / 2.0\n",
    "        X = X.astype(int)\n",
    "\n",
    "    print('accessing', j, manyruns_path)\n",
    "    X_multi[j, :, :] = X[0:nn, 0:kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP aligned needs a relationdict for the 'time varying' dataset\n",
    "# our relation is that each traj maps to itself (in time) -- constant relation\n",
    "\n",
    "constant_dict = {i: i for i in range(kk)}\n",
    "constant_relations = [constant_dict for i in range(len(gamma_list)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_multi_as_list = [X_multi[i,:,:] for i in range(X_multi.shape[0])]\n",
    "aligned_mapper = umap.AlignedUMAP().fit(X_multi, relations=constant_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def axis_bounds(embedding):\n",
    "    left, right = embedding.T[0].min(), embedding.T[0].max()\n",
    "    bottom, top = embedding.T[1].min(), embedding.T[1].max()\n",
    "    adj_h, adj_v = (right - left) * 0.1, (top - bottom) * 0.1\n",
    "    return [left - adj_h, right + adj_h, bottom - adj_v, top + adj_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 4\n",
    "num_cols = 4\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, 40))\n",
    "ax_bound = axis_bounds(np.vstack(aligned_mapper.embeddings_))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    if i<len(gamma_list):\n",
    "        #current_target = ordered_target[150 * i:min(ordered_target.shape[0], 150 * i + 400)]\n",
    "        \n",
    "        manyruns_path = manyruns_paths[i]\n",
    "        print(i, gamma_list[i], manyruns_path)\n",
    "        fpath_energy = manyruns_path + os.sep + 'aggregate' + os.sep + 'X_energy_last.npz'\n",
    "        X_energies = np.load(fpath_energy)['arr_0'].T  # umap wants transpose (?)\n",
    "        energies = X_energies[:nn, 0]\n",
    "        \n",
    "        #ax.scatter(*aligned_mapper.embeddings_[i].T, s=2, cmap=\"Spectral_r\")\n",
    "        ax.scatter(*aligned_mapper.embeddings_[i].T, s=10, c=energies, cmap=\"Spectral_r\")\n",
    "        \n",
    "        ax.axis(ax_bound)\n",
    "        ax.set(xticks=[], yticks=[])\n",
    "plt.tight_layout()\n",
    "plt.savefig('aligned%d_gammas%d.jpg' % (nn, len(gamma_list)), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picks = [14, 11, 10, 9, 7, 6, 5, 0]\n",
    "picks = [0, 1]\n",
    "\n",
    "num_rows = 1\n",
    "num_cols = len(picks)\n",
    "\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(30, 10))\n",
    "ax_bound = axis_bounds(np.vstack(aligned_mapper.embeddings_))\n",
    "for j, ax in enumerate(axs.flatten()):\n",
    "    if j<len(picks):\n",
    "        \n",
    "        i = picks[j]\n",
    "        #current_target = ordered_target[150 * i:min(ordered_target.shape[0], 150 * i + 400)]\n",
    "        \n",
    "        manyruns_path = manyruns_paths[i]\n",
    "        print(i, gamma_list[i], manyruns_path)\n",
    "        fpath_energy = manyruns_path + os.sep + 'aggregate' + os.sep + 'X_energy_last.npz'\n",
    "        X_energies = np.load(fpath_energy)['arr_0'].T  # umap wants transpose (?)\n",
    "        energies = X_energies[:nn, 0]\n",
    "        \n",
    "        #ax.scatter(*aligned_mapper.embeddings_[i].T, s=2, cmap=\"Spectral_r\")\n",
    "        ax.set_title(gamma_list[i], fontsize=20)\n",
    "        sc = ax.scatter(*aligned_mapper.embeddings_[i].T, s=10, c=energies, cmap=\"Spectral_r\")\n",
    "        \n",
    "        if i == 1:\n",
    "            cbar = plt.colorbar(sc)\n",
    "            cbar.ax.tick_params(size=0)\n",
    "\n",
    "        ax.axis(ax_bound)\n",
    "        ax.set(xticks=[], yticks=[])\n",
    "plt.tight_layout()\n",
    "plt.savefig('picks_aligned%d_gammas%d.jpg' % (nn, len(gamma_list)), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#picks = [14, 11, 10, 9, 7, 6, 5, 1]\n",
    "#picks = [13, 11, 10, 9, 7, 6, 2, 1]\n",
    "#picks = [13, 9, 7, 6, 4, 3, 1, 0]\n",
    "picks = [13, 7, 6, 5, 4, 3, 1, 0]\n",
    "picks = [15, 14, 13, 7, 6, 5, 4, 3, 2, 1, 0]\n",
    "#picks = [14, 11, 10, 9, 7, 6, 5, 0]\n",
    "#picks = [0]\n",
    "\n",
    "print(\"Max picks:\", len(manyruns_paths) - 1)\n",
    "print(\"Picks:\", [gamma_list[p] for p in picks])\n",
    "\n",
    "\n",
    "square = True\n",
    "\n",
    "num_rows = 1\n",
    "num_cols = len(picks)\n",
    "\n",
    "if square:\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(30, 7))  # SQUARE 30 x 5\n",
    "else:\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(30, 10))  # TALL 30 x 10\n",
    "\n",
    "ax_bound = axis_bounds(np.vstack(aligned_mapper.embeddings_))\n",
    "for j, ax in enumerate(axs.flatten()):\n",
    "    if j<len(picks):\n",
    "        \n",
    "        i = picks[j]\n",
    "        #current_target = ordered_target[150 * i:min(ordered_target.shape[0], 150 * i + 400)]\n",
    "        \n",
    "        manyruns_path = manyruns_paths[i]\n",
    "        print(i, gamma_list[i], manyruns_path)\n",
    "        fpath_energy = manyruns_path + os.sep + 'aggregate' + os.sep + 'X_energy_last.npz'\n",
    "        X_energies = np.load(fpath_energy)['arr_0'].T  # umap wants transpose (?)\n",
    "        energies = X_energies[:nn, 0]\n",
    "        \n",
    "        #ax.scatter(*aligned_mapper.embeddings_[i].T, s=2, cmap=\"Spectral_r\")\n",
    "        #ax.set_title(gamma_list[i], fontsize=30, y=None)\n",
    "        if square:\n",
    "            ax.set_title(gamma_list[i], fontsize=30, y=-0.12)\n",
    "            sc = ax.scatter(*aligned_mapper.embeddings_[i].T, s=5, c=energies, cmap=\"Spectral_r\")\n",
    "        else:\n",
    "            ax.set_title(gamma_list[i], fontsize=30, y=-0.06)\n",
    "            sc = ax.scatter(*aligned_mapper.embeddings_[i].T, s=10, c=energies, cmap=\"Spectral_r\")\n",
    "\n",
    "        \n",
    "        #cbar = plt.colorbar(sc)\n",
    "        #cbar.ax.tick_params(size=0)\n",
    "\n",
    "        #ax.axis(ax_bound)\n",
    "        ax.set(xticks=[], yticks=[])\n",
    "plt.tight_layout()\n",
    "plt.savefig('picks_nobound_aligned%d_gammas%d.jpg' % (nn, len(gamma_list)), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#picks = [14, 11, 10, 9, 7, 6, 5, 1]\n",
    "#picks = [14, 11, 10, 9, 7, 6, 5, 1]\n",
    "picks = [13, 7, 6, 5, 4, 3, 1, 0]\n",
    "#picks = [14, 11, 10, 9, 7, 6, 5, 0]\n",
    "#picks = [0]\n",
    "\n",
    "#picks = [13, 12, 11, 10, 9, 8, 7, 0]#, 4, 3, 1, 0]\n",
    "#picks = picks[::-1]\n",
    "\n",
    "square = True\n",
    "\n",
    "num_rows = 2\n",
    "num_cols = 4\n",
    "\n",
    "if square:\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(18*0.68, 12*0.68))  # SQUARE 30 x 5\n",
    "else:\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(30, 10))  # TALL 30 x 10\n",
    "\n",
    "ax_bound = axis_bounds(np.vstack(aligned_mapper.embeddings_))\n",
    "for j, ax in enumerate(axs.flatten()):\n",
    "    if j<len(picks):\n",
    "        \n",
    "        i = picks[j]\n",
    "        #current_target = ordered_target[150 * i:min(ordered_target.shape[0], 150 * i + 400)]\n",
    "        \n",
    "        manyruns_path = manyruns_paths[i]\n",
    "        print(i, gamma_list[i], manyruns_path)\n",
    "        fpath_energy = manyruns_path + os.sep + 'aggregate' + os.sep + 'X_energy_last.npz'\n",
    "        X_energies = np.load(fpath_energy)['arr_0'].T  # umap wants transpose (?)\n",
    "        energies = X_energies[:nn, 0]\n",
    "        \n",
    "        #ax.scatter(*aligned_mapper.embeddings_[i].T, s=2, cmap=\"Spectral_r\")\n",
    "        #ax.set_title(gamma_list[i], fontsize=30, y=None)\n",
    "        if square:\n",
    "            ax.set_title(gamma_list[i], fontsize=20, y=-0.12)\n",
    "            sc = ax.scatter(*aligned_mapper.embeddings_[i].T, s=2, c=energies, cmap=\"Spectral_r\")\n",
    "        else:\n",
    "            ax.set_title(gamma_list[i], fontsize=30, y=-0.06)\n",
    "            sc = ax.scatter(*aligned_mapper.embeddings_[i].T, s=10, c=energies, cmap=\"Spectral_r\")\n",
    "\n",
    "        \n",
    "        #cbar = plt.colorbar(sc)\n",
    "        #cbar.ax.tick_params(size=0)\n",
    "\n",
    "        #ax.axis(ax_bound)\n",
    "        ax.set(xticks=[], yticks=[])\n",
    "plt.tight_layout()\n",
    "plt.savefig('picks_nobound_2x4_aligned%d_gammas%d.jpg' % (nn, len(gamma_list)), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single 2D figure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "\n",
    "pick = 1\n",
    "\n",
    "use_plotly = True\n",
    "\n",
    "manyruns_path = manyruns_paths[pick]\n",
    "print(pick, gamma_list[pick], manyruns_path)\n",
    "fpath_energy = manyruns_path + os.sep + 'aggregate' + os.sep + 'X_energy_last.npz'\n",
    "X_energies = np.load(fpath_energy)['arr_0'].T  # umap wants transpose (?)\n",
    "energies = X_energies[:nn, 0]\n",
    "\n",
    "X = aligned_mapper.embeddings_[pick]\n",
    "print(X.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "# ===============================\n",
    "if use_plotly:\n",
    "    clabel = 'energies'\n",
    "    df = pd.DataFrame({'index': range(nn),\n",
    "                        clabel: energies,\n",
    "                       'x': X[:, 0],\n",
    "                       'y': X[:, 1],\n",
    "                       'z': energies})\n",
    "\n",
    "    fig = px.scatter(df, x='x', y='y',\n",
    "                     color=clabel,\n",
    "                     title='jupyter',\n",
    "                     hover_name='index')\n",
    "    fig.write_html('simple2d_pick%d.html' % pick)\n",
    "    fig.show()\n",
    "else:\n",
    "    ax = fig.add_subplot(111)\n",
    "    #ax.scatter(X[:,0], X[:,1], c=energies, s=20, cmap='Spectral_r')\n",
    "    \n",
    "    #ax.scatter(X[:,0], X[:,1], c=energies, s=10, cmap='Spectral_r')\n",
    "    ax.scatter(X[:,0], X[:,1], c=None, s=10, cmap='Spectral_r')\n",
    "\n",
    "    #ax.set_xlabel('X Label')\n",
    "    #ax.set_ylabel('Y Label')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.savefig('ttt.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single 2D+E landscape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "\n",
    "pick = 1\n",
    "\n",
    "use_plotly = True\n",
    "\n",
    "manyruns_path = manyruns_paths[pick]\n",
    "print(pick, gamma_list[pick], manyruns_path)\n",
    "fpath_energy = manyruns_path + os.sep + 'aggregate' + os.sep + 'X_energy_last.npz'\n",
    "X_energies = np.load(fpath_energy)['arr_0'].T  # umap wants transpose (?)\n",
    "energies = X_energies[:nn, 0]\n",
    "\n",
    "X = aligned_mapper.embeddings_[pick]\n",
    "print(X.shape)\n",
    "\n",
    "print(X[2602,:])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "# ===============================\n",
    "if use_plotly:\n",
    "    clabel = 'energies'\n",
    "    df = pd.DataFrame({'index': range(nn),\n",
    "                        clabel: energies,\n",
    "                       'x': X[:, 0],\n",
    "                       'y': X[:, 1],\n",
    "                       'z': energies})\n",
    "\n",
    "    fig = px.scatter_3d(df, x='x', y='y', z='z',\n",
    "                        color=clabel,\n",
    "                        title='jupyter',\n",
    "                        hover_name='index')\n",
    "    fig.show()\n",
    "else:\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(X[:,0], X[:,1], energies, c=energies, cmap='Spectral_r')\n",
    "    ax.set_xlabel('X Label')\n",
    "    ax.set_ylabel('Y Label')\n",
    "    ax.set_zlabel('Z Label')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot specific points from index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multicell.unsupervised_helper import plot_given_multicell\n",
    "\n",
    "\n",
    "agg_indices = [911]\n",
    "outdir = RUNS_FOLDER + os.sep + 'explore' + os.sep + 'plot_specific_points'\n",
    "\n",
    "# where is the data?\n",
    "step = None\n",
    "#dirname = 'Wrandom0_gamma0.20_10k_periodic_fixedorderV3_p3_M100'\n",
    "dirname = 'Wrandom0_gamma1.00_10k_periodic_fixedorderV3_p3_M100'\n",
    "\n",
    "\n",
    "#step = 14\n",
    "#dirname = 'beta2.05_Wrandom0_gamma0.20_10k_periodic_fixedorderV3_p3_M100'\n",
    "\n",
    "manyruns_path = RUNS_FOLDER + os.sep + 'multicell_manyruns' + os.sep + dirname\n",
    "fpath_pickle = manyruns_path + os.sep + 'multicell_template.pkl'\n",
    "with open(fpath_pickle, 'rb') as pickle_file:\n",
    "    multicell = pickle.load(pickle_file)  # unpickling multicell object\n",
    "\n",
    "for agg_index in agg_indices:  \n",
    "    #smod = ''\n",
    "    smod = '_last'\n",
    "    if step is not None:\n",
    "        smod = '_%d' % step\n",
    "    \n",
    "    agg_dir = manyruns_path + os.sep + 'aggregate'\n",
    "    fpath_state = agg_dir + os.sep + 'X_aggregate%s.npz' % smod\n",
    "    fpath_energy = agg_dir + os.sep + 'X_energy%s.npz' % smod\n",
    "    fpath_pickle = manyruns_path + os.sep + 'multicell_template.pkl'\n",
    "    print(fpath_state)\n",
    "    X = np.load(fpath_state)['arr_0'].T  # umap wants transpose\n",
    "    X_state = X[agg_index, :]\n",
    "    \n",
    "    step_hack = 0  # TODO care this will break if class has time-varying applied field\n",
    "    multicell.graph_state_arr[:, step_hack] = X_state[:]\n",
    "    #assert np.array_equal(multicell_template.field_applied, np.zeros((total_spins, multicell_template.total_steps)))\n",
    "    plot_given_multicell(multicell, step_hack, agg_index, outdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check indiv lattice states**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_io import run_subdir_setup, RUNS_FOLDER, INPUT_FOLDER\n",
    "from multicell.graph_helper import state_load\n",
    "from multicell.graph_adjacency import lattice_square_loc_to_int\n",
    "from singlecell.singlecell_simsetup import singlecell_simsetup\n",
    "\n",
    "replot_dir = RUNS_FOLDER + os.sep + 'explore' + os.sep + 'replot'\n",
    "\n",
    "sidelength = 20\n",
    "curated = True\n",
    "random_mem = False        # TODO incorporate seed in random XI in simsetup/curated\n",
    "random_W = False          # TODO incorporate seed in random W in simsetup/curated\n",
    "W_override_path = INPUT_FOLDER + os.sep + 'manual_WJ' + os.sep + 'simsetup_W_9_maze.txt'\n",
    "simsetup_main = singlecell_simsetup(\n",
    "    unfolding=True, random_mem=random_mem, random_W=random_W, curated=curated, housekeeping=0)\n",
    "if W_override_path is not None:\n",
    "    print('Note: in main, overriding W from file...')\n",
    "    explicit_W = np.loadtxt(W_override_path, delimiter=',')\n",
    "    simsetup_main['FIELD_SEND'] = explicit_W\n",
    "print(\"simsetup checks:\")\n",
    "print(\"\\tsimsetup['N'],\", simsetup_main['N'])\n",
    "print(\"\\tsimsetup['P'],\", simsetup_main['P'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = [a for a in os.listdir(replot_dir) if a[-4:] == '.npz']\n",
    "fpaths = [replot_dir + os.sep + a for a in fnames]\n",
    "print(fpaths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fpaths[9])\n",
    "X = state_load(fpaths[9], cells_as_cols=True, num_genes=None, num_cells=None, txt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = (0,-3)\n",
    "node_idx = lattice_square_loc_to_int(loc, sidelength)\n",
    "cellstate = X[:, node_idx]\n",
    "print(cellstate)\n",
    "print(np.dot(simsetup_main['XI'].T, cellstate)/9.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turquoise = [30, 223, 214]\n",
    "\n",
    "white = [255,255,255]\n",
    "soft_grey = [225, 220, 222]\n",
    "soft_grey_alt1 = [206, 199, 182]\n",
    "soft_grey_alt2 = [219, 219, 219]\n",
    "beige = [250, 227, 199]\n",
    "\n",
    "soft_blue = [148, 210, 226]\n",
    "soft_blue_alt1 = [58, 128, 191]\n",
    "\n",
    "soft_red = [192, 86, 64]\n",
    "soft_red_alt1 = [240, 166, 144]\n",
    "soft_red_alt2 = [255, 134, 113]\n",
    "\n",
    "soft_yellow = [237, 209, 112]\n",
    "\n",
    "soft_orange = [250, 173, 63]\n",
    "soft_orange_alt1 = [248, 200, 140]\n",
    "\n",
    "soft_green = [120, 194, 153]\n",
    "sharp_green = [142, 200, 50]\n",
    "\n",
    "soft_purple = [177, 156, 217]\n",
    "\n",
    "soft_grey_norm = np.array(soft_grey) / 255.0\n",
    "\n",
    "color_anchor_beige = np.array(beige) / 255.0\n",
    "color_anchor_white = np.array(white) / 255.0\n",
    "color_anchor = color_anchor_white\n",
    "\n",
    "\n",
    "#color_A_pos = np.array(soft_blue_alt1) / 255.0\n",
    "color_A_pos = np.array(soft_blue) / 255.0\n",
    "color_A_neg = np.array(soft_orange) / 255.0\n",
    "\n",
    "color_B_pos = np.array(soft_red) / 255.0\n",
    "color_B_neg = np.array(soft_green) / 255.0\n",
    "\n",
    "color_C_pos = np.array(soft_yellow) / 255.0\n",
    "color_C_neg = np.array(soft_purple) / 255.0\n",
    "\n",
    "def linear_interpolate(val, c2, c1=color_anchor):\n",
    "    eps = 1e-4\n",
    "    assert 0.0 <= val <= 1.0 + eps\n",
    "    cout = c1 + val * (c2 - c1)\n",
    "    return cout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_arr_color(color):\n",
    "    q = np.zeros((10,10,3))\n",
    "    q[:,:,0] += color[0]\n",
    "    q[:,:,1] += color[1]\n",
    "    q[:,:,2] += color[2]\n",
    "    return q\n",
    "\n",
    "fig, axarr = plt.subplots(1,3)\n",
    "a = np.array([color_A_pos]).reshape(1,1,3)\n",
    "axarr[0].imshow(a)\n",
    "b = np.array([color_B_pos]).reshape(1,1,3)\n",
    "axarr[1].imshow(b)\n",
    "c = np.array([color_C_pos]).reshape(1,1,3)\n",
    "axarr[2].imshow(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(1,3)\n",
    "colour_mix = linear_interpolate(0.3, color_A_pos, c1=color_B_pos)\n",
    "a = np.array([colour_mix]).reshape(1,1,3)\n",
    "axarr[0].imshow(a)\n",
    "\n",
    "colour_mix = linear_interpolate(0.3, color_A_pos, c1=color_C_pos)\n",
    "b = np.array([colour_mix]).reshape(1,1,3)\n",
    "axarr[1].imshow(b)\n",
    "\n",
    "colour_mix = linear_interpolate(0.3, color_B_pos, c1=color_C_pos)\n",
    "c = np.array([colour_mix]).reshape(1,1,3)\n",
    "axarr[2].imshow(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(soft_purple)/255.0\n",
    "b = np.array(soft_green)/255.0\n",
    "c = np.array(soft_orange)/255.0\n",
    "\n",
    "amount = np.sqrt(0.11)\n",
    "\n",
    "fig, axarr = plt.subplots(1,3)\n",
    "colour_mix = linear_interpolate(amount, a, c1=color_anchor_white)\n",
    "axarr[0].imshow(np.array([colour_mix]).reshape(1,1,3))\n",
    "\n",
    "colour_mix = linear_interpolate(amount, b, c1=color_anchor_white)\n",
    "axarr[1].imshow(np.array([colour_mix]).reshape(1,1,3))\n",
    "\n",
    "colour_mix = linear_interpolate(amount, c, c1=color_anchor_white)\n",
    "axarr[2].imshow(np.array([colour_mix]).reshape(1,1,3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual UMAP plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})\n",
    "\n",
    "from singlecell.singlecell_linalg import sorted_eig\n",
    "from utils.file_io import RUNS_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multicell.unsupervised_helper import make_dimreduce_object, save_dimreduce_object\n",
    "\n",
    "def plotly_express_embedding_LOCAL(data_subdict, color_by_index=False, as_landscape=False,\n",
    "                             fmod='', show=False, dirpath=None, surf=False, step=None):\n",
    "    \"\"\"\n",
    "    Supports 2D and 3D embeddings\n",
    "    color_by_index: for troubleshooting, colors the points according to their array position\n",
    "        if False (default), color by energy instead\n",
    "    \"\"\"\n",
    "    # colormaps here: https://plotly.com/python/builtin-colorscales/\n",
    "    fmod += '_jupyter'\n",
    "\n",
    "    num_runs = data_subdict['num_runs']\n",
    "    label = data_subdict['label']\n",
    "    if dirpath is None:\n",
    "        dirpath = data_subdict['path'] + os.sep + 'dimreduce'\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "    smod = ''\n",
    "    if step is not None:\n",
    "        smod = ' (step %d)' % step\n",
    "\n",
    "    if color_by_index:\n",
    "        c = np.arange(num_runs)\n",
    "        fmod += '_cIndex'\n",
    "        clabel = 'index'\n",
    "    else:\n",
    "        c = data_subdict['energies'][:, 0]  # range(num_runs)\n",
    "        clabel = 'energy'\n",
    "\n",
    "    for key, algodict in data_subdict['algos'].items():\n",
    "        algo = key\n",
    "        embedding = algodict['embedding']\n",
    "\n",
    "        n_components = embedding.shape[1]\n",
    "        assert n_components in [2, 3]\n",
    "\n",
    "        plot_title = '%s of %s dataset%s' % (algo, label, smod)\n",
    "        plot_path = dirpath + os.sep + \"%s_plotly_%s%s\" % (algo, label, fmod)\n",
    "\n",
    "        if not as_landscape:\n",
    "            if n_components == 2:\n",
    "                df = pd.DataFrame({'index': range(num_runs),\n",
    "                                   clabel: c,\n",
    "                                   'x': embedding[:, 0],\n",
    "                                   'y': embedding[:, 1]})\n",
    "\n",
    "                fig = px.scatter(df, x='x', y='y',\n",
    "                                 color=clabel,\n",
    "                                 title=plot_title,\n",
    "                                 hover_name='index')\n",
    "                fig.update_layout({\n",
    "                    'plot_bgcolor': 'rgba(0,0,0,0)',\n",
    "                    'paper_bgcolor': 'rgba(0,0,0,0)'})\n",
    "\n",
    "            else:\n",
    "                df = pd.DataFrame({'index': range(num_runs),\n",
    "                                   clabel: c,\n",
    "                                   'x': embedding[:, 0],\n",
    "                                   'y': embedding[:, 1],\n",
    "                                   'z': embedding[:, 2]})\n",
    "\n",
    "                fig = px.scatter_3d(df, x='x', y='y', z='z',\n",
    "                                    color=clabel,\n",
    "                                    title=plot_title,\n",
    "                                    hover_name='index')\n",
    "\n",
    "        else:\n",
    "            plot_title += ' landscape'\n",
    "            plot_path += '_landscape'\n",
    "            df = pd.DataFrame({'index': range(num_runs),\n",
    "                               clabel: c,\n",
    "                               'x': embedding[:, 0],\n",
    "                               'y': embedding[:, 1],\n",
    "                               'z': data_subdict['energies'][:, 0]})\n",
    "            if surf:\n",
    "                plot_title += ' surface'\n",
    "                plot_path += 'Surf'\n",
    "\n",
    "                # SKETCHY: assumes Z = X * Y in shape\n",
    "                # - will make Z = all zeros except z_i on diag\n",
    "                \"\"\"\n",
    "                xx = df['x']\n",
    "                yy = df['y']\n",
    "                zz = df['z']\n",
    "\n",
    "                xx = xx[0:1000]\n",
    "                yy = yy[0:1000]\n",
    "                zz = zz[0:1000]\n",
    "\n",
    "                zmax = np.max(zz)\n",
    "                buffer = 0.1 * np.abs(zmax)\n",
    "                zmax += buffer\n",
    "                Z = np.zeros((xx.size, yy.size))\n",
    "                np.fill_diagonal(Z, zz)\n",
    "\n",
    "                fig = go.Figure(data=[go.Surface(\n",
    "                    z=Z, x=zz, y=yy)\n",
    "                ])\n",
    "                fig.update_layout(title=plot_title)\n",
    "                \"\"\"\n",
    "                # Regular trisurf approach (ugly)\n",
    "                u = embedding[:, 0]\n",
    "                v = embedding[:, 1]\n",
    "\n",
    "                from scipy.spatial import Delaunay\n",
    "\n",
    "                points2D = np.vstack([u, v]).T\n",
    "                tri = Delaunay(points2D)\n",
    "                simplices = tri.simplices\n",
    "\n",
    "                fig = ff.create_trisurf(\n",
    "                    x=df['x'], y=df['y'], z=df['z'],\n",
    "                    colormap=\"Thermal\",\n",
    "                    simplices=simplices,\n",
    "                    title=plot_title)\n",
    "\n",
    "            else:\n",
    "                fig = px.scatter_3d(df, x='x', y='y', z='z',\n",
    "                                    color=clabel,\n",
    "                                    title=plot_title,\n",
    "                                    hover_name='index')\n",
    "\n",
    "        fig.write_html(plot_path + '.html')\n",
    "        fig.write_image(plot_path + '.png')\n",
    "        if show:\n",
    "            fig.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these set the defaults for modifications introduced in main\n",
    "REDUCER_SEED = 100\n",
    "REDUCER_COMPONENTS = 3\n",
    "#REDUCERS_TO_USE = ['pca']\n",
    "#REDUCERS_TO_USE = ['tsne']\n",
    "REDUCERS_TO_USE = ['umap']\n",
    "#REDUCERS_TO_USE = ['umap', 'tsne', 'pca']\n",
    "#VALID_REDUCERS = ['umap', 'tsne', 'pca']\n",
    "\n",
    "# see defaults: https://umap-learn.readthedocs.io/en/latest/api.html\n",
    "UMAP_KWARGS = {\n",
    "    'random_state': REDUCER_SEED,\n",
    "    'n_components': REDUCER_COMPONENTS,\n",
    "    'metric': 'euclidean',\n",
    "    'init': 'spectral',\n",
    "    'unique': False,\n",
    "    'n_neighbors': 15,\n",
    "    'min_dist': 0.1,\n",
    "    'spread': 1.0,\n",
    "}\n",
    "TSNE_KWARGS = {\n",
    "    'random_state': REDUCER_SEED,\n",
    "    'n_components': REDUCER_COMPONENTS,\n",
    "    'metric': 'euclidean',\n",
    "    'init': 'random',\n",
    "    'perplexity': 30.0,\n",
    "}\n",
    "PCA_KWARGS = {\n",
    "    'n_components': REDUCER_COMPONENTS,\n",
    "}\n",
    "\n",
    "\n",
    "# main flags\n",
    "build_dimreduce_dicts = True\n",
    "add_control_data = False\n",
    "vis_all = True\n",
    "pca_assess = False\n",
    "plot_specific_points = False\n",
    "check_evals = False\n",
    "\n",
    "# data process settings6\n",
    "use_01 = True\n",
    "jitter_scale = 0  #1e-4\n",
    "nsubsample = None  # None or an int\n",
    "\n",
    "# Step 0) which 'manyruns' dirs to work with\n",
    "#gamma_list = [0.0, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.15, 0.20, 0.4, 0.6, 0.8, 0.9, 1.0, 20.0]\n",
    "gamma_list = [20.0]\n",
    "\n",
    "#gamma_list = [0.0, 0.2]\n",
    "# gamma_list = [2.0, 20.0]\n",
    "\n",
    "step_list = [None]\n",
    "# step_list = [0.0, 10.0]  # list of [None] or list of steps\n",
    "#step_list = [0, 1, 2, 3] + list(np.arange(4, 20, 5))\n",
    "#step_list = [0, 1, 2]\n",
    "#step_list = [0] + list(range(4, 30, 5))\n",
    "#step_list = list(range(0, 10, 1))\n",
    "\n",
    "#manyruns_dirnames = ['Wrandom0_gamma%.2f_10k_p3_M100' % a for a in gamma_list]\n",
    "#manyruns_dirnames = ['Wrandom0_gamma%.2f_10k_fixedorderNotOrig_p3_M100' % a for a in gamma_list]\n",
    "#manyruns_dirnames = ['Wrandom1_gamma%.2f_10k_fixedorder_p3_M100' % a for a in gamma_list]\n",
    "manyruns_dirnames = ['Wrandom0_gamma%.2f_10k_periodic_fixedorderV3_p3_M100' % a for a in gamma_list]\n",
    "\n",
    "manyruns_paths = [RUNS_FOLDER + os.sep + 'multicell_manyruns' + os.sep + dirname\n",
    "                  for dirname in manyruns_dirnames]\n",
    "\n",
    "# Step 1) umap (or other dim reduction) kwargs\n",
    "if any([build_dimreduce_dicts, add_control_data, vis_all, pca_assess]):\n",
    "    for n_components in [2]:\n",
    "\n",
    "        for step in step_list:\n",
    "            #n_components = 3\n",
    "            pca_kwargs = PCA_KWARGS.copy()\n",
    "            pca_kwargs['n_components'] = n_components  # TODO don't need to spec 'live', can embed later?\n",
    "            umap_kwargs = UMAP_KWARGS.copy()\n",
    "            umap_kwargs['n_components'] = n_components  # TODO don't need to spec 'live', can embed later?\n",
    "            tsne_kwargs = TSNE_KWARGS.copy()\n",
    "            tsne_kwargs['n_components'] = n_components  # TODO don't need to spec 'live', can embed later?\n",
    "            # modify pca settings\n",
    "            # modify umap settings\n",
    "            #umap_kwargs['unique'] = True\n",
    "            #umap_kwargs['n_neighbors'] = 100\n",
    "            umap_kwargs['min_dist'] = 0.25\n",
    "            umap_kwargs['spread'] = 1.0\n",
    "            #umap_kwargs['metric'] = 'euclidean'\n",
    "            # modify tsne settings\n",
    "            #tsne_kwargs['perplexity'] = 100\n",
    "\n",
    "            # Modify filename suffix for dimreduce pkl and plots\n",
    "            fmod = ''\n",
    "            if step is not None:\n",
    "                fmod += '_step%d' % step\n",
    "            fmod += '_F=' + '+'.join(REDUCERS_TO_USE)\n",
    "            fmod += '_dim%d_seed%d' % (umap_kwargs['n_components'],\n",
    "                                       umap_kwargs['random_state'])\n",
    "            if use_01:\n",
    "                fmod += '_use01'\n",
    "            if nsubsample is not None:\n",
    "                fmod += '_nn%d' % nsubsample\n",
    "            if jitter_scale > 0:\n",
    "                fmod += '_jitter%.4f' % jitter_scale\n",
    "            if 'umap' in REDUCERS_TO_USE:\n",
    "                if umap_kwargs['metric'] != 'euclidean':\n",
    "                    fmod += '_%s' % umap_kwargs['metric']\n",
    "                if umap_kwargs['init'] != 'spectral':\n",
    "                    fmod += '_%s' % umap_kwargs['init']\n",
    "                if umap_kwargs['n_neighbors'] != 15:\n",
    "                    fmod += '_nbor%d' % umap_kwargs['n_neighbors']\n",
    "                if umap_kwargs['min_dist'] != 0.1:\n",
    "                    fmod += '_dist%.2f' % umap_kwargs['min_dist']\n",
    "                if umap_kwargs['spread'] != 1.0:\n",
    "                    fmod += '_spread%.2f' % umap_kwargs['spread']\n",
    "                if umap_kwargs['unique']:\n",
    "                    fmod += '_unique'\n",
    "            if 'tsne' in REDUCERS_TO_USE:\n",
    "                if tsne_kwargs['perplexity'] != 30.0:\n",
    "                    fmod += '_perplex%.2f' % tsne_kwargs['perplexity']\n",
    "\n",
    "            # Step 2) make/load data\n",
    "            datasets = {i: {'label': manyruns_dirnames[i],\n",
    "                            'path': manyruns_paths[i]}\n",
    "                        for i in range(len(manyruns_dirnames))}\n",
    "\n",
    "            for idx in range(len(manyruns_dirnames)):\n",
    "                fpath = manyruns_paths[idx] + os.sep + 'dimreduce' \\\n",
    "                        + os.sep + 'dimreduce%s.z' % fmod\n",
    "                if os.path.isfile(fpath):\n",
    "                    print('Exists already, loading: %s' % fpath)\n",
    "                    fcontents = joblib.load(fpath)  # just load file if it exists\n",
    "                    datasets[idx] = fcontents\n",
    "                else:\n",
    "                    print('Dim. reduction on manyruns: %s' % manyruns_dirnames[idx])\n",
    "                    datasets[idx] = make_dimreduce_object(\n",
    "                        datasets[idx], nsubsample=nsubsample, flag_control=False,\n",
    "                        use_01=True, jitter_scale=jitter_scale,\n",
    "                        umap_kwargs=umap_kwargs, tsne_kwargs=tsne_kwargs, pca_kwargs=pca_kwargs,\n",
    "                        step=step)\n",
    "                    save_dimreduce_object(datasets[idx], fpath)  # save to file (joblib)\n",
    "\n",
    "            if add_control_data:\n",
    "                print('adding control data...')\n",
    "                total_spins_0 = datasets[0]['total_spins']\n",
    "                num_runs_0 = datasets[0]['num_runs']\n",
    "\n",
    "                # add control data into the dict of datasets\n",
    "                control_X = generate_control_data(total_spins_0, num_runs_0)\n",
    "                control_folder = RUNS_FOLDER + os.sep + 'multicell_manyruns' + os.sep + 'control'\n",
    "                control_fpath = control_folder + os.sep + \\\n",
    "                                'dimreduce' + os.sep + 'dimreduce%s.z' % fmod\n",
    "\n",
    "                datasets[-1] = {\n",
    "                    'data': control_X,\n",
    "                    'label': 'control (coin-flips)',\n",
    "                    'num_runs': num_runs_0,\n",
    "                    'total_spins': total_spins_0,\n",
    "                    'energies': np.zeros((num_runs_0, 5)),\n",
    "                    'path': control_folder\n",
    "                }\n",
    "                datasets[-1] = make_dimreduce_object(\n",
    "                    datasets[-1], flag_control=True,\n",
    "                    nsubsample=nsubsample, jitter_scale=jitter_scale, use_01=use_01,\n",
    "                    umap_kwargs=umap_kwargs, tsne_kwargs=tsne_kwargs, pca_kwargs=pca_kwargs)\n",
    "                save_dimreduce_object(datasets[-1], control_fpath)  # save to file (joblib)\n",
    "\n",
    "            # Step 3) vis data\n",
    "            if vis_all:\n",
    "                for idx in range(0, len(manyruns_dirnames)):\n",
    "                    plotly_express_embedding_LOCAL(\n",
    "                        datasets[idx], fmod=fmod, show=False,\n",
    "                        step=step)\n",
    "                    plotly_express_embedding_LOCAL(\n",
    "                        datasets[idx], fmod=fmod, color_by_index=True, show=False,\n",
    "                        step=step)\n",
    "                    plotly_express_embedding_LOCAL(\n",
    "                        datasets[idx], fmod=fmod, as_landscape=True, show=False,\n",
    "                        step=step)\n",
    "                    #plotly_express_embedding(\n",
    "                    #    datasets[idx], fmod=fmod, as_landscape=True, show=False, surf=True)\n",
    "                    if pca_assess:\n",
    "                        pca_assess_dataset(datasets[idx], fmod=fmod, show=False)\n",
    "\n",
    "                if add_control_data:\n",
    "                    plotly_express_embedding_LOCAL(datasets[-1], fmod=fmod, color_by_index=True)\n",
    "                    if pca_assess:\n",
    "                        pca_assess_dataset(datasets[-1], fmod=fmod, show=False)\n",
    "\n",
    "            # Step 3) plot special indices of the multicell state\n",
    "            if plot_specific_points:\n",
    "                #agg_indices = [2611, 2289]\n",
    "                agg_indices = [481, 4774]\n",
    "                outdir = RUNS_FOLDER + os.sep + 'explore' + os.sep + 'plot_specific_points'\n",
    "\n",
    "                for idx in range(0, len(manyruns_dirnames)):\n",
    "\n",
    "                    multicell = datasets[idx]['multicell_template']\n",
    "\n",
    "                    for agg_index in agg_indices:\n",
    "                        # pull relevant info from subdict\n",
    "                        X = datasets[idx]['data'][agg_index, :]\n",
    "                        step_hack = 0  # TODO care this will break if class has time-varying applied field\n",
    "                        multicell.graph_state_arr[:, step_hack] = X[:]\n",
    "                        #assert np.array_equal(multicell_template.field_applied, np.zeros((total_spins, multicell_template.total_steps)))\n",
    "                        plot_given_multicell(multicell, step_hack, agg_index, outdir)\n",
    "\n",
    "# Step 4) eval check of Jij\n",
    "if check_evals:\n",
    "    for idx, dirpath in enumerate(manyruns_paths):\n",
    "        fpath_pickle = dirpath + os.sep + 'multicell_template.pkl'\n",
    "        with open(fpath_pickle, 'rb') as pickle_file:\n",
    "            multicell_template = pickle.load(pickle_file)  # unpickling multicell object\n",
    "\n",
    "        J_multicell = multicell_template.matrix_J_multicell\n",
    "        evals, evecs = sorted_eig(J_multicell, take_real=True)\n",
    "        plt.scatter(range(len(evals)), evals)\n",
    "        plt.title(r'Spectrum of $J_{\\mathrm{multicell}}$ for: %s' % os.path.basename(dirpath))\n",
    "        plt.xlabel('rank of $\\lambda$')\n",
    "        plt.ylabel('$\\lambda$')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def axis_bounds(embedding):\n",
    "    left, right = embedding.T[0].min(), embedding.T[0].max()\n",
    "    bottom, top = embedding.T[1].min(), embedding.T[1].max()\n",
    "    adj_h, adj_v = (right - left) * 0.1, (top - bottom) * 0.1\n",
    "    return [left - adj_h, right + adj_h, bottom - adj_v, top + adj_v]\n",
    "\n",
    "num_rows = 2\n",
    "num_cols = 8\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 10))\n",
    "ax_bound = axis_bounds(\n",
    "    np.vstack( [datasets[i]['algos']['umap']['embedding'] for i in range(len(gamma_list))] )\n",
    ")\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    if i<len(gamma_list):\n",
    "        print(i)\n",
    "        #current_target = ordered_target[150 * i:min(ordered_target.shape[0], 150 * i + 400)]\n",
    "        Xd = datasets[i]['algos']['umap']['embedding']\n",
    "        energies = datasets[i]['energies'][:,0]\n",
    "        print(Xd.shape)\n",
    "        ax.set_title(gamma_list[i])\n",
    "        #ax.scatter(Xd[:,0], Xd[:,1], s=1, cmap=\"Spectral\")\n",
    "        ax.scatter(Xd[:,0], Xd[:,1], s=1, c=energies, cmap=\"Spectral_r\")\n",
    "        ax.axis(ax_bound)\n",
    "        ax.set(xticks=[], yticks=[])\n",
    "    else:\n",
    "        fig.delaxes(ax)\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "#plt.savefig('aligned%d_gammas%d.jpg' % (nn, len(gamma_list)))\n",
    "plt.savefig('Subplots%d%s.jpg' % (len(gamma_list), fmod))\n",
    "#plt.savefig('Subplots%d%s.pdf' % (len(gamma_list), fmod))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 7))\n",
    "ax = plt.gca()\n",
    "\n",
    "i = 3\n",
    "Xd = datasets[i]['algos']['umap']['embedding']\n",
    "energies = datasets[i]['energies'][:,0]\n",
    "print(Xd.shape)\n",
    "ax.set_title(gamma_list[i])\n",
    "\n",
    "#ax.scatter(Xd[:,0], Xd[:,1], s=1, cmap=\"Spectral\")\n",
    "#ax.scatter(Xd[:,0], Xd[:,1], s=1, c=energies, cmap=\"RdYlBu_r\")\n",
    "sc = ax.scatter(Xd[:,0], Xd[:,1], s=5, c=energies, cmap=\"Spectral_r\", alpha=1.0, edgecolor='k', linewidths=0.0)\n",
    "#cbar = plt.colorbar(sc)\n",
    "cbar.ax.tick_params(size=0)\n",
    "\n",
    "#ax.axis(ax_bound)\n",
    "ax.set(xticks=[], yticks=[])\n",
    "#plt.tight_layout()\n",
    "#plt.savefig('Subplots%d%s.jpg' % (len(gamma_list), fmod))\n",
    "#plt.savefig('Subplots%d%s.pdf' % (len(gamma_list), fmod))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_rows = 2\n",
    "num_cols = 9\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, 10))\n",
    "#ax_bound = axis_bounds(\n",
    "#    np.vstack( [datasets[i]['algos']['umap']['embedding'] for i in range(len(gamma_list))] )\n",
    "#)\n",
    "\n",
    "\n",
    "picks = list(range(len(gamma_list)))\n",
    "picks = [0, 3, 4, 6, 7, 8, 9, 11, 13]\n",
    "\n",
    "for j, ax in enumerate(axs.flatten()):\n",
    "    if j < len(picks):\n",
    "        print(j)\n",
    "        i = picks[j]\n",
    "        #current_target = ordered_target[150 * i:min(ordered_target.shape[0], 150 * i + 400)]\n",
    "        Xd = datasets[i]['algos']['umap']['embedding']\n",
    "        energies = datasets[i]['energies'][:,0]\n",
    "        print(Xd.shape)\n",
    "        ax.set_title(gamma_list[i])\n",
    "        #ax.scatter(Xd[:,0], Xd[:,1], s=1, cmap=\"Spectral\")\n",
    "        ax.scatter(Xd[:,0], Xd[:,1], s=1, c=energies, cmap=\"Spectral_r\")\n",
    "        #ax.axis(ax_bound)\n",
    "        ax.set(xticks=[], yticks=[])\n",
    "    else:\n",
    "        fig.delaxes(ax)\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "#plt.savefig('aligned%d_gammas%d.jpg' % (nn, len(gamma_list)))\n",
    "plt.savefig('PicksSubplots%d%s.jpg' % (len(gamma_list), fmod), dpi=300)\n",
    "#plt.savefig('Subplots%d%s.pdf' % (len(gamma_list), fmod))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
